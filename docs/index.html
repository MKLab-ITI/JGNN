<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JGNN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-vs.min.css"
        integrity="sha512-Jn4HzkCnzA7Bc+lbSQHAMeen0EhSTy71o9yJbXZtQx9VvozKVBV/2zfR3VyuDFIxGvHgbOMMNvb80l+jxFBC1Q=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        body {
            display: flex;
            overflow-x: hidden; /* Prevent horizontal scrolling */
        }
		
		.table td, .table th {
		  min-width: 100px; /* Adjust the value as needed */
		}

        #sidebar {
            position: fixed;
            top: 0;
            left: 0;
            height: 100%;
            width: 190px;
            background-color: #f8f9fa;
            padding-top: 20px;
            padding-left: 5px;
            border-right: 1px solid #ddd;
        }

        .sidebar-title {
            text-align: center;
        }

        #content {
            margin-left: 200px;
            padding: 20px;
            flex-grow: 1;
            overflow-wrap: break-word;
            word-wrap: break-word;
            word-break: break-word;
            width: 100%; /* Ensures content takes full width */
        }

        section {
            margin-top: 90px;
        }

        .sidebar-link {
            color: #000;
            text-decoration: none;
            padding: 10px;
            display: block;
        }

        .sidebar-link.active {
            color: #fff;
            background-color: #007bff;
            border-radius: 4px;
        }

        @media (max-width: 767.98px) {
            #sidebar {
                display: none;
            }

            .navbar {
                position: fixed;
                display: block;
                color: #fff;
                background-color: #007bff;
                border-radius: 4px;
            }

            #content {
                margin-left: 0;
                padding: 20px;
                flex-grow: 1;
                overflow-wrap: break-word;
                word-wrap: break-word;
                word-break: break-word;
            }
        }

        @media (min-width: 768px) {
            .navbar {
                display: none;
            }
        }
    </style>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            const sidebarLinks = document.querySelectorAll('.sidebar-link');

            function removeActiveClasses() {
                sidebarLinks.forEach(link => link.classList.remove('active'));
            }

            function addActiveClass(link) {
                removeActiveClasses();
                link.classList.add('active');
            }

            const options = {
                root: null,
                rootMargin: '0px',
                threshold: 0.2
            };

            const observerCallback = (entries, observer) => {
                entries.forEach(entry => {
                    const id = entry.target.getAttribute('id');
                    const link = document.querySelector(`.sidebar-link[href="#${id}"]`);

                    if (entry.isIntersecting) {
                        addActiveClass(link);
                    }
                });
            };

            const observer = new IntersectionObserver(observerCallback, options);

            document.querySelectorAll('section').forEach(section => {
                observer.observe(section);
            });
        });
    </script>
</head>

<body>
    <nav id="sidebar">
        <ul class="nav flex-column">
            <li class="nav-item sidebar-title"><h5>JGNN</h5></li>
            <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#modelbuilder"
                    style="color: #777777;">3.1. ModelBuilder</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#fastbuilder"
                    style="color: #777777;">3.2. FastBuilder</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#debugging"
                    style="color: #777777;">3.4. Debugging</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#neuralang">4. Neuralang</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#create-data">5. Create data</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#fill-in-data"
                    style="color: #777777;">5.1. Fill in data</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#identifiers"
                    style="color: #777777;">5.2. Identifiers</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection"
                    href="#tensor-operations" style="color: #777777;">4.3. Tensor operations</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#training">6. Training</a></li>
        </ul>
    </nav>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <div class="container-fluid">
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
					<li class="nav-item"> <a class="sidebar-link" href="#neuralang">4. Neuralang</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#create-data">5. Create data</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#training">6. Training</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <div id="content">
		<h1 style="text-align: center;">JGNN</h1>
		<p>Graph Neural Networks (GNNs) are getting more and more popular, for example to
		make predictions based on relational information, or to perform inference
		on small datasets. JGNN provides cross-platform implementations of this machine
		learning paradigm that do not require dedicated hardware or firmware. While
		reading this guidebook, keep in mind that this is not a library for running
		computationally intensive architectures; it has no GPU support and does not
		plan to add any (unless such support becomes integrated in the Java virtual 
		machine). Instead, the goal is to provide highly portable solutions
		that can run under smaller compute and available memory. So, while complex 
		architectures like gated attention networks with many layers and hidden dimensions 
		can be created using the libary, running them fastly may require compromises
		in terms of the number of learned parameters or computational complexity.
		The forte of JGNN lies is porting more lightweight counterparts in applications
		grappling with limited resources.</p>
		
	<section id="setup">
	<h1>1. Setup</h1>
	<p>The simplest way to set up JGNN is to download it as JAR package from
	the project's <a href="https://github.com/MKLab-ITI/JGNN/releases">releases</a> 
	and add it in a Java project's dependencies. However, those working with Maven 
	or Gradle can also add JGNN's latest release as a dependency from the JitPack 
	repository. Follow the link below for full instructions.<br>
	<a href="https://jitpack.io/#MKLab-ITI/JGNN"><img src="https://jitpack.io/v/MKLab-ITI/JGNN.svg" alt="download JGNN"></a>
	</p>
	<p>
	For instance, the fields in the snippet below may be added in a maven <em>.pom</em> file 
	to work with the latest nightly release.</p>
	<pre><code class="language-xml">&lt;repositories&gt;
	&lt;repository&gt;
		&lt;id&gt;jitpack.io&lt;/id&gt;
		&lt;url&gt;https://jitpack.io&lt;/url&gt;
	&lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
	&lt;dependency&gt;
		&lt;groupId&gt;com.github.MKLab-ITI&lt;/groupId&gt;
		&lt;artifactId&gt;JGNN&lt;/artifactId&gt;
		&lt;version&gt;v1.3.20-nightly&lt;/version&gt;
	&lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
	</section>
	
	<section id="quickstart">
	<h1>2. Quickstart</h1>

	<p>Here we demonstrate usage of JGNN for node classification, that is, the inductive learning
	task of inferring node labels given a graph's structure, node features, and a few node labels.
	Classifying graphs is also supported, but it is a harder task to explain and set up. 
	GNN architectures for the chosen node classification task are typically written 
	as message-passing mechanisms; these diffuse node representations across edges, where
	node neighbors pick up, aggregate (e.g., average), and transform
	incoming representations to update theirs. Alternatives that boast higher 
	expressive power also exist and are supported, but simple architectures 
	may be just as good or better than complex alternatives in solving 
	practical problems <a href="https://www.mdpi.com/2076-3417/14/11/4533">[Krasanakis et al., 2024]</a>.
	Simple architectures also enjoy reduced resource consumption.</p>
	
	<p>The demonstration starts by loading the <code class="language-java">Cora</code> dataset from those shipped 
	with the library for out-of-the-box testing. The first time this dataset is 
	constructed, it automatically downloads some data and stores them in a local <code class="language-java">downloads/</code>
	folder. The data are then loaded into a sparse graph adjacency matrix, a dense node feature matrix, 
	and a dense node label matrix. Sparse and dense representations are interchangeable in terms of operations,
	but sparse matrices can store graphs with many nodes but relatively smaller degrees with greater memory efficiency. 
	In the loaded matrices, each row contains the corresponding node's
	neighbors, features, or one-hot encoding of labels. We also apply the renormalization trick and symmetric normalization on 
	the dataset using in-place operations. The first of these makes GNN computations numerically stable by adding self-loops 
	to all nodes, whereas the second is required by the model we impelement next.</p>
	
	<pre><code class="language-java">Dataset dataset = new Cora();
dataset.graph().setMainDiagonal(1).setToSymmetricNormalization();</code></pre>
	
	
	<p>We now incrementally construct a trainable model using symbolic definitions resembling math 
	notation. Symbolic expressions are part of a scripting-like language, called Neuralang,
	that is covered in <a href="#neuralang">section 4</a>. However, for faster onboarding we stick to
	the <code class="language-java">FastBuilder</code> class for creating models. This class's constructor
	creates two constants <code>A</code> and <code>h0</code> from its two arguments to. Other constants
	and input variables can be set too, but more on this later. After instantiation, we use the
	following model builder methods to constuct a model. Some of these methods parse symbolic expressions 
	to fastly declare machine learning components.
	<ul>
		<li><code>config</code> - Configures hyperparameter values. These can be used in all subsequent function and layer declarations.</li>
		<li><code>function</code> - Declares a Neuralang function, in this case with inputs <code>A</code> and <code>h</code>.</li>
		<li><code>layer</code> - Declares a layer that can use builtin and Neuralang functions. In this, the symbols <code>{l}</code> and <code>{l+1}</code> specifically are replaced by a layer counter.</li>
		<li><code>classify</code> - Adds a softmax layer tailored to classification. This also silently declares an input <code>nodes</code> that represents a list of node indices where the outputs should be computed.</li>
		<li><code>autosize</code> - Automatically sizes matrix and vector dimensions filled by <code>?</code>. This requires some input example, and here we provide a list of node identifiers, which we also make dataless (have only the correct dimensions without allocating memory). This method also checks for integrity errors in the declared architecture, such as computational paths that do not lead to an output.</li>
	</ul>
	The abode methods support a method chain paradigm where the modelBuilder instance is returned by each of 
	its methods to access the next one. Below we use this builder to implement the Graph Convolutional Network (GCN) 
	architecture <a href="https://arxiv.org/abs/1609.02907">[Kipf and Welling, 2017]</a>.
	Details on the symbolic parts of definitions are presented later but, for the time being, we point to usage 
	of the <code>matrix</code> and <code>vector</code> builtins to inline declarations of learnable parameter for
	given dimensions and regularization. 
	The builder stores internally a constructed model, obtained through <code>modelBuilder.getModel()</code>.
	</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new FastBuilder(dataset.graph(), dataset.features())
	.config("reg", 0.005)
	.config("classes", numClasses)
	.config("hidden", 64)
	.function("gcnlayer", "(A,h){Adrop = dropout(A, 0.5); return Adrop@(h@matrix(?, hidden, reg))+vector(?);}")
	.layer("h{l+1}=relu(gcnlayer(A, h{l}))")
	.config("hidden", "classes")  // reassigns the output gcnlayer's "hidden" to be the number of "classes"
	.layer("h{l+1}=gcnlayer(A, h{l})")
	.classify()
	.autosize(new EmptyTensor(numSamples));</code></pre>
	
	
	<p>Training epochs for the constructed model can be implemented
	manually, by passing inputs, obtaining outputs, computing losses, and triggering backpropagation
	on an optimizer. However, this may be complicated to write properly, so JGNN automates common 
	training patterns with a <code>ModelTraining</code> class. Instances of this class
	accept a method chain notation to set their parameters, like the number of epochs, patience 
	for early stopping, the employed optimizer, and loss functions. An example is presented below,
	where <code>Adam</code> optimization with learning rate <i>0.01</i> is performed, and a verbose 
	variation of a validation loss keeps track of training progress. To run a full training process,
	the defined strategy is passed to the model alongside input data, corresponding output data, as well
	as training and validation slices.</p>
	<p>Notice how, before training starts, a parameter initializer is also applied on the model for a cold
	start. Selecting an initilizer is not part of training strategies 
	to signify its model-dependent nature; dense layers should maintain the expected 
	input variances in the output before the first epoch. Moreover,
	the graph's adjacency matrix and node features are already declared as constants by the 
	<code>FastBuilder</code> constructor, as node classification takes place on the same graph
	with fully known node features. Instead, what is considered as inputs and outputs in this case
	are the node identifiers (which in the <code>classify</code> method above are used to gather
	the outputs of respective nodes) and corresponding labels. Labels that are not known (e.g., because
	they refer to test data) still need to have some value, so as a convention if your are working
	with your own data leave the one-hot label encoding of test nodes as zeroes. Doing so in this
	example would not affect the outcome either. To recap, our full dataset consists of all node
	identifiers and corresponding labels. The last two arguments of the <code>train</code> method
	then accept training and validation data slices. Slices are effectively lists of integer entries 
	pointing to rows of the datasets - find more later.
	</p>
	<pre><code class="language-java">ModelTraining trainer = new ModelTraining()
	.setOptimizer(new Adam(0.01))
	.setEpochs(3000)
	.setPatience(100)
	.setLoss(new CategoricalCrossEntropy())
	.setValidationLoss(new VerboseLoss(new Accuracy()).setInterval(10));  // print validation every 10 epochs
	
Slice nodes = dataset.samples().getSlice().shuffle(); // a permutation of node identifiers
Matrix inputData = Tensor.fromRange(nodes.size()).asColumn(); // each node has its identifier as an input
Model model = modelBuilder.getModel()
		.init(new XavierNormal())
		.train(trainer, 
				inputData,
				dataset.labels(), 
				nodes.range(0, 0.6),  // training slice
				nodes.range(0.6, 0.8)  // validation slice
				);</code></pre>
	
	<p>Trained models and their generating builders can be saved and loaded. These next snippet demonstrates
	how raw predictions can be made too. During this process,
	some matrix manipulation operations are employed to obtain transparent access to parts of input and output data
	of the dataset.
	</p>

	<pre><code class="language-java">modelBuilder.save(Paths.get("gcn_cora.jgnn")); // needs a Path as an input
Model loadedModel = ModelBuilder.load(Paths.get("gcn_cora.jgnn")).getModel(); // loading creates an intermediate modelbuilder

Matrix output = loadedModel.predict(Tensor.fromRange(0, nodes.size()).asColumn()).get(0).cast(Matrix.class);
double acc = 0;
for(Long node : nodes.range(0.8, 1)) {
	Matrix nodeLabels = dataset.labels().accessRow(node).asRow();
	Tensor nodeOutput = output.accessRow(node).asRow();
	acc += nodeOutput.argmax()==nodeLabels.argmax()?1:0;
}
System.out.println("Acc\t "+acc/nodes.range(0.8, 1).size());</code></pre>
	
    </section>
		

	<section id="gnn-builders">
	<h1>3. GNN Builders</h1>
	<p>We already touched on the subject of GNN architecture builders when the Neuralang language
	was first introduced in the introductory demonstration. To recap, there are different kinds of
	builders, some of which do not implement all features of the language in favor of
	simplifying parts of architecture definitions with hard-coded Java implementations. 
	Here we cover the base GNNBuilder class that can only parse simple expressions, 
	the FastBuilder class that introduces node classification boilerplate code, 
	and the Neuralang class that parses the full language, including function definitions
	and handling configurations through expressions.</p>
	
	<h3 id="modelbuilder">3.1. ModelBuilder</h3>
	[Under construction]
	
	
	
	
	you can use within expressions. Unless otherwise specified, you can replace <code>x</code> and <code>y</code> with any expression. Sometimes, <code>y</code> needs to be a constant defined either by presenting a number, calling <code>ModelBuilder.config(y, double)</code>, or calling <code>ModelBuilder.constant(y, double)</code> to set the numbers as hyperparameters.</p>

    <table class="table" border=" 1">
            <tr>
                <th>Symbol</th>
                <th>Type</th>
                <th>Number of inputs</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><code>x = y</code></td>
                <td>Operator</td>
                <td>Assign to variable <code>x</code> the outcome of executing <code>y</code>.</td>
            </tr>
            <tr>
                <td><code>x + y</code></td>
                <td>Operator</td>
                <td>Element-by-element addition.</td>
            </tr>
            <tr>
                <td><code>x * y</code></td>
                <td>Operator</td>
                <td>Element-by-element multiplication.</td>
            </tr>
            <tr>
                <td><code>x - y</code></td>
                <td>Operator</td>
                <td>Element-by-element subtraction.</td>
            </tr>
            <tr>
                <td><code>x @ y</code></td>
                <td>Operator</td>
                <td>Matrix multiplication.</td>
            </tr>
            <tr>
                <td><code>x | y</code></td>
                <td>Operator</td>
                <td>Row-wise concatenation of <code>x</code> and <code>y</code>.</td>
            </tr>
            <tr>
                <td><code>x [y]</code></td>
                <td>Operator</td>
                <td>Gathers the rows of <code>x</code> with indexes <code>y</code>.</td>
            </tr>
            <tr>
                <td><code>transpose(x)</code></td>
                <td>Function</td>
                <td>Transposes matrix <code>x</code>.</td>
            </tr>
            <tr>
                <td><code>log(x)</code></td>
                <td>Function</td>
                <td>Apply logarithm on each tensor element.</td>
            </tr>
            <tr>
                <td><code>relu(x)</code></td>
                <td>Function</td>
                <td>Apply relu on each tensor element.</td>
            </tr>
            <tr>
                <td><code>tanh(x)</code></td>
                <td>Function</td>
                <td>Apply a tanh activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>sigmoid(x)</code></td>
                <td>Function</td>
                <td>Apply a sigmoid activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>dropout(x, rate)</code></td>
                <td>Function</td>
                <td>Apply training dropout on tensor <code>x</code> with constant dropout rate hyperparameter <code>rate</code>.</td>
            </tr>
            <tr>
                <td><code>lrelu(x, slope)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with constant negative slope hyperparameter <code>slope</code>.</td>
            </tr>
            <tr>
                <td><code>prelu(x)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with learnable negative slope.</td>
            </tr>
            <tr>
                <td><code>softmax(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a softmax reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>sum(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a sum reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>sum(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a sum reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>max(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a max reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>min(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a min reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols, reg)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter size <code>len</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len, reg)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter <code>len</code>, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
        </tbody>
    </table>

    <p>Prefer using hyperparameters (set via <code>.config</code>) for matrix and vector creation, as these transfer their names to respective dimensions for error checking. For <code>dropout</code>, <code>matrix</code>, and <code>vector</code> you can also use the short names <code>drop</code>, <code>mat</code>, <code>vec</code>.</p>

	<h3 id="fastbuilder">3.2. FastBuilder</h3>
	<p>The <code class="language-java">FastBuilder</code> class for building GNN architectures extends the generic
	<code class="language-java">LayerBuilder</code> with common graph neural network operations. The main difference 
	is that it is initialized with a square matrix <code class="language-java">A</code>, which is typically expected to
	be a normalization of the (sparse) adjacency matrix, and a feature matrix <code class="language-java">h0</code>. 
	This builder annotates layer representations with <code class="language-java">h{l}</code> for the last layer
	and <code class="language-java">h{l+1}</code> for the next one, where the layer counter symbol <code class="language-java">{l}</code>
	is incremented by one when, instead of simply parsing an expression, the <code class="language-java">.layer(String)</code>
	method is used. Before tackling layer definitions, the builder is instantiated in the way shown below.
	Be careful that sending specific tensors to the builder's constructor still allows external
	edits. 
	</p>

	<pre><code class="language-java">FastBuilder modelBuilder = new FastBuilder(adjacency, features);</code></pre>

	<p>The base operation of message passing GNNs, which are often used for node classification,
	is to propagate node representations to neighbors via graph edges. Then, neighbors aggregate
	the received representation, where aggregation typically consists of a weighted average per 
	the normalized adjacency matrix's edge weights. For symmetric normalization, this the
	weighted sum is compatible with spectral graph signal processing. The operation to perform
	one propagation can be written as <code class="language-java">.layer("h{l+1}=A @ h{l}")</code>.
	However, the propagation's outcome is often further transformed further by passing through a dense
	layer. This is demonstrated in the following snippet, where two layers of propagation and
	dense transformation are stacked. Usage of the layer counter symbol <code class="language-java">{l}</code> lets
	the written code read similarly to what you would find in a paper.</p>

	<pre><code class="language-java">modelBuilder
	.layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden, reg))+vector(hidden))")
	.layer("h{l+1}=A@(h{l}@matrix(hidden, classes, reg))+vector(classes)");</code></pre>
	
	<p>Many architectures also perform edge dropout, which is as simple as applying dropout on the adjacency
	matrix on each layer like so: <code class="language-java">.layer("h{l+1}=dropout(A,0.5) @ h{l}")</code>.
	However, performance gains of this practice tend to be minimal in research papers, and we recommend
	ignoring this practice in favor of faster computations; although only non-zero
	elements of sparse matrices are

	<p>Recent areas of heterogeneous graph research explicitly
	use the graph Laplacian, which you can insert into the
	architecture as a normal constant like so: <code class="language-java">.constant("L", adjacency.negative().cast(Matrix.class).setMainDiagonal(1))</code>
	More complex concepts can be modeled with edge attention,
	which gathers and performs the dot product of edge nodes
	to provide new edge weights. This yields an adjacency
	matrix weighting unique to the layer per: <code class="language-java">.operation("A{l} = L1(nexp(att(A, h{l})))")</code>.
	However, it is recommended to stay away from these kinds
	of complex architectures when learning from large
	graphs, as JGNN is designed to be lightweight and not
	fast. Consider using other systems to learn complex GNNs in GPUs 
	if 1-2% accuracy gains are worth the loss of portability.</p>
	
	
	
	<p>So far, we have discussed the propagation mechanisms of
	GNNs, which consider the features of all nodes. However,
	in a node classification setting, training data labels
	are typically available only for certain nodes. We thus
	need a mechanism to retrieve the predictions of the top
	neural layer for certain nodes and pass them through a
	softmax activation.</p>

	<p>This can be achieved with normal neural model definitions
	using the gather bracket operation after declaring a
	variable of which nodes to retrieve:</p>

	<pre><code class="language-java">.var("nodes")
.layer("h{l} = softmax(h{l})")
.operation("output = h{l}[nodes]")</code></pre>

	<p>Recall that <code class="language-java">h{l}</code> always points to the top
	layer when writing a new layer. This way, the built
	model takes as inputs a set of nodes, performs the
	forward pass of the architecture, and then selects the
	provided nodes to use as outputs (and backpropagate
	from). <strong>All</strong> nodes are needed for
	training because they are made aware of each other via
	the graph's structure. 
	To simplify how node classification architectures are
	defined, the above symbolic snippet is automatically
	generated and applied by calling the
	<code class="language-java">.classify()</code> method of the
	<code class="language-java">FastBuilder</code> instead.
	</p>

	
	
	<h3 id="debugging">3.4. Debugging</h3>
	<p>JGNN offers high-level tools for debugging
		architectures. Here we cover what errors to
		expect, what diagnostics to run, and how to make
		sense of error messages to fix erroneous
		architectures. First, When parsing operations,
		values should be assigned to variables before
		subsequent use. Model builders, including the
		NeuraLang builder, check for unused
		variables and raise respective runtime
		exceptions. For example, consider a <code class="language-java">FastBuilder</code> that
		tries to parse the following expression, where <code class="language-java">hl</code> is a typographical error of
		<code class="language-java">h{l}</code>. An exception is thrown.</p>

	<pre><code class="language-java">.layer("h{l+1}=relu(hl@matrix(features, 32, reg)+vector(32))")</code></pre>
	<pre><code class="language-java">Exception in thread "main" java.lang.RuntimeException: Symbol hl not defined.</code></pre>

	<p>Model builders are responsible for creating
		and managing directed acyclic graphs (DAGs) in models 
		they are creating. DAGs are not to be confused with graph
		inputs GNNs manage, but are just an organization of
		Java components. During parsing, builders
		may create temporary variables, which start with
		the <code class="language-java">_tmp</code> prefix and are followed by
		a number. Temporary variables often link
		components to other that use them.
		The easiest way to understand execution DAGs is
		to look at them. The library provides two tools
		for this purpose: a <code class="language-java">.print()</code> method
		that prints built functional flows in the system
		console, and a <code class="language-java">.getExecutionGraphDot()</code> 
		method that returns a String holding the execution graph in
		<em>.dot</em> format for visualization with
		tools like <a href="https://dreampuf.github.io/GraphvizOnline">GraphViz</a>.
	</p>

	<p>Another error-checking procedure consists of
		checking for model operations that do not
		eventually reach any outputs, for example, one
		of the output operation outcomes defined by
		<code class="language-java">.out(String)</code>. Avoiding this
		behavior is important as it messes with graph
		traversal counting during backpropagation.
		To accommodate complex use cases, these
		checks are manually performed, and expected at
		the end of model building with the
		method <code class="language-java">.assertBackwardValidity()</code>
		that throws an exception if an invalid model is found.
		Performing this assertion early on in
		model building will likely throw exceptions that
		are not logical errors, given that independend
		outputs may be combined later. Backward validity errors 
		look like this the following example. This 
		indicates that the component
		<code class="language-java">_tmp102</code> does not lead to an output,
		and we should look at the execution tree to
		understand its role.</p>

	<pre><code class="language-java">Exception in thread "main" java.lang.RuntimeException: The component class mklab.JGNN.nn.operations.Multiply: _tmp102 = null does not lead to an output
at mklab.JGNN.nn.ModelBuilder.assertBackwardValidity(ModelBuilder.java:504)
at nodeClassification.APPNP.main(APPNP.java:45)</code></pre>

	
			<p>In addition to other operations, some methods
			do not affect tensor or matrix values but
			are only responsible for naming dimensions.
			Functionally, these are decorative and aim
			to improve debugging by throwing errors for
			incompatible non-null names. For example,
			adding two matrices with different dimension
			names will result in an error. Likewise, the
			inner dimension names during matrix
			multiplication should agree. 
			Arithmetic operations, <em>including</em>
			matrix multiplication and copying,
			automatically infer dimension names in the
			result to ensure only compatible data types
			are compared. Dimension names can be freely
			changed for any Tensor <em>without</em>
			backtracking changes (even for see-through
			data types, such as the outcome of
			<code class="language-java">asTransposed()</code>).
			Matrices effectively have three
			dimension names: for their rows, columns,
			and inner data as long as they are treated
			as tensors.</p>

			<table class="table table-bordered">
				<thead>
					<tr>
						<th>Operation</th>
						<th>Type</th>
						<th>Comments</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><code class="language-java">Tensor setDimensionName(String name)</code></td>
						<td>arithmetic</td>
						<td>For naming tensor dimensions (of
							the 1D space tensors lie
							in).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setRowName(String rowName)</code></td>
						<td>arithmetic</td>
						<td>For naming what kind of
							information matrix rows hold
							(e.g., "samples").</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setColName(String colName)</code></td>
						<td>arithmetic</td>
						<td>For naming what kind of
							information matrix columns hold
							(e.g., "features").</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setDimensionName(String rowName, String colName)</code></td>
						<td>arithmetic</td>
						<td>A shorthand of calling
							<code class="language-java">setRowName(rowName).setColName(colName)</code>.
						</td>
					</tr>
				</tbody>
			</table>

<p>There are two main mechanisms for identifying
logically erroneous architectures: a) mismatched
dimension size, and b) mismatched dimension
names. Of the two, dimension sizes are easier to
comprehend since they just mean that operations
are mathematically invalid.</p>

<p>On the other hand, dimension names need to be
determined for starting data, such as model
inputs and parameters, and are automatically
inferred from operations on such primitives. For
in-line declaration of parameters in operations
or layers, dimension names are copied from any
hyperparameters. Therefore, for easier
debugging, prefer using functional expressions
that declare hyperparameters:</p>

<pre><code class="language-java">new ModelBuilder()
	.config("features", 7)
	.config("hidden", 64)
	.var("x")
	.operation("h = x@matrix(features, hidden)");</code></pre>

<p>instead of the simpler:</p>

<pre><code class="language-java">new ModelBuilder().var(x).operation("h = x@matrix(features, hidden)")</code></pre>

<p>Both mismatched dimensions and mismatched
	dimension names throw runtime exceptions. The
	beginning of their error console traces should
	start with something like this:</p>

<pre><code class="language-java">java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null with the following inputs:
class mklab.JGNN.nn.activations.Relu: h1 = SparseMatrix (3327,32) 52523/106464 entries
class mklab.JGNN.nn.inputs.Parameter: _tmp5 = DenseMatrix (64, classes 6)
java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
at mklab.JGNN.core.Matrix.matmul(Matrix.java:258)
at mklab.JGNN.nn.operations.MatMul.forward(MatMul.java:21)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:180)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
...</code></pre>

<p>For example, this tells us that the architecture
	encounters mismatched matrix sizes when trying
	to multiply a 3327x32 SparseMatrix with a 64x6
	dense matrix. Understanding the exact error is
	easy—the inner matrix dimensions of matrix
	multiplication do not agree. However, we need to
	find the error within our architecture to fix
	it. To do this, the error message message states 
	that the error occures.
	<code>During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null</code>. This tells us that the problem
	occurs when trying to calculate
	<code>_tmp4</code>, which is currently assigned
	a <code class="language-java">null</code> tensor as value. Some more
	information is available to see what the
	operation's inputs are like—in this case, they
	coincide with the multiplication's inputs, but
	this will not always be the case.
	The important point is to go back to the
	execution tree and see during which exact
	operation this variable is defined. There, we
	will undoubtedly find that some dimension had 64
	instead of 32 elements or conversely.</p>

	<p>In addition to all other debugging mechanisms,
	JGNN presents a way to show when forward and
	backward operations of specific code components
	are executed and with what kinds of arguments.
	This can be particularly useful when testing new
	components in real (complex) architectures.
	The practice consists of calling a
	<code class="language-java">monitor(...)</code> function within
	operations. This does not affect what
	expressions do and only enables printing
	execution tree operations on operation
	components. For example, the next snippet
	monitors the outcome of matrix multiplication:
	</p>
	<pre><code class="language-java">builder.operation("h = relu(monitor(x@matrix(features, 64)) + vector(64))")</code></pre>
		
	</section>
	
	<section id="neuralang">
	<h1>4. Neuralang</h1>
	
	<h5>The language</h5>
	<p>Neuralang scripts consist of functions that declare machine learning
	components and their interactions using a syntax inspired by the Mojo. 
	language. Use a Rust highlighter to cover all keywords. 
	Before explaining how to use the <code>Neuralang</code> model builder,
	we first analyse some of the functions in the Quickstart section code 
	to explain the lenguage's syntax. First, let us look at <code>classify</code> 
	function that takes 
	two inputs: <code>nodes</code> that correspond to node identifiers 
	for classification, and a node feature matrix <code>h</code>. 
	A softmax is returned for the features of the specified nodes. The function's 
	signature also has several configuration values, whose defaults 
	are indicated by a colon <code>:</code>. The same notation is used to 
	set/overwrite configurations when calling functions, as we do for softmax 
	to apply it row-wise. Think of configurations as keyword 
	arguments of typical programming languages, with the difference that
	they control hyperparameters, like dimension sizes or regularization.
	Write exact values for configurations, as for now there no 
	arithmetics take place for them. For example, a configuration 
	<code>patience:2*50</code> creates an error.</p>
		
	<pre><code class="language-rust">fn classify(nodes, h, epochs: !3000, patience: !100, lr: !0.01) {
	return softmax(h[nodes], dim: "row");
}</code></pre>

	<p>Exclamation marks <code>!</code> before numbers broadcast values
	to all subsequent function calls that have configurations with the same 
	name. The broadcasted defaults overwrite default values. Importantly, 
	broadcasted values are also retrievable from JGNN's Neuralang model 
	builder too; which is useful for Java integration. In this case, values
	for the training process are obtained (see below). Configuration 
	values have the priority:<br> 
	1. function call arguments<br>
	2. broacasted configurations (the last broadcasted value, including configurations set by Java)<br>
	3. function signature defaults<br>
	</p>
	
	<p>Next, let us look at the <code>gcnlayer</code> function. This accepts 
	two parameters: an adjacency matrix <code>A</code> and input feature matrix <code>h</code>.
	The configuration <code>hidden: 64</code> in the functions's signature 
	specifies the deafult number of hidden units,
	whereas <code>reg: 0.005</code> is the L2 regularization applied 
	during machine learning. The questionmark <code>?</code> 
	in matrix definitions lets the autosize feature of JGNN determine 
	dimension sizes based on a test run - if possible. 
	Finally, the function returns the activated output of a
	GCN layer. 
	
	<pre><code class="language-rust">fn gcnlayer(A, h, hidden: 64, reg: 0.005) {
	return A@h@matrix(?, hidden, reg) + vector(hidden);
}</code></pre>
	
	<p>The last of the functions we tackle, the gcn function declares the popular Graph Convoluational Network (GCN) 
	architecture and has as configuration the number of output classes. 
	The function first applies a gcnlayer, and then applies another layer 
	of the same type with the hidden units configuration set to the value of classes. 
	Thus the output matches the number of classes, which is set as externally declared 
	with the <code>extern</code> keyword; this signifies that the configuration has no default
	and should be provided by Java's side of the implementation.</p>
	
	
	
	<h5>Java-side integration</h5>
	
	

	<p>We now need generate a trainable model by incrementally constructing model builders.
	These differ by which Neuralang capabilities they can parse - mainly whether they go beyond
	support of simple expressions involving operators only. The builder chosen here supports the whole 
	language, but in turn misses out on some Java-side methods that help fill parts of the architecture
	with boilerplate patterns. To use the selected builder, save the first Neuralang snippet 
	to a file and retrieve it with the 
	expression <code class="language-java">String architecture = Paths.get("filename.nn");</code>.
	(Use a Rust language highlighter for visual assistance when writting in Neuralang.)
	Alternatively, avoid external files by inlining the definition within Java code through 
	a multiline string per <code class="language-java">String architecture = """ ... """;</code>.
	This string is parsed within a functional programming chain, where
	each method call returns the modelBuilder instance to continue calling more methods. 
	</p>
	
	
	<p>For our model builder, we set remaining hyperparameters and overwrite the default value
	for <code class="language-java">"hidden"</code> using the 
	<code class="language-java">.config(String, double)</code> method. We also determine
	which variables are constants, namely the adjacency matrix <code class="language-java">A</code> and node 
	representation <code class="language-java">h</code>, and that node identifiers is a variable that serves 
	as the architecture's inputs. There could be multiple inputs, so this distinction of what 
	is a constant and what is a variable depends mostly on the which quantities change 
	during training. In the case of node classification, both the adjacency matrix and
	node features remain constant, as we work in one graph. Finally, the definition
	sets an Neuralang expression as the architecture's output
	by calling the <code class="language-java">.out(String)</code> method,
	and applies the <code class="language-java">.autosize(Tensor...)</code> method to infer hyperparameter
	values denoted with <code class="language-java">?</code> from an example input (for faster inference, we 
	provide dataless list of node identifiers as input).</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new Neuralang()
	.parse(architecture)
	.constant("A", dataset.graph())
	.constant("h", dataset.features())
	.var("nodes")
	.config("classes", numClasses)
	.config("hidden", numClasses+2)
	.out("ngcn(A, h, nodes)")
	.autosize(new EmptyTensor(numSamples));</code></pre>
		
    </section>

	

</section>

	<h3>3.4. Generalized message passing</h3>
	<p>Similarly, JGNN also supports the the fully generized
	message-passing scheme between node neighbors to support
	complex types of relational analysis, such as edge attributes.
	However, this option may be computationally intense compared 
	to simpler alternatives. In this generalization,
	each edge is responsible for appropriately
	transforming and propagating representations to
	node neighbors. This process can capture various
	settings of interest
	<a href="https://arxiv.org/pdf/2202.11097.pdf">[Velickovic, 2022]</a>.
	In the more general sense, messages in GNNs can
	be matrices whose rows correspond to edges and
	columns to edge features. In the simplest scenario, you 
	can create such matrices by gathering the features of edge 
	source anddestination nodes by accessing the respective
	elements of a feature matrix
	<code class="language-java">h{l}</code>. To do this, first
	obtain edge source indexes
	<code class="language-java">src=from(A)</code> and destination indexes
	<code class="language-java">dst=to(A)</code> where <code class="language-java">A</code> is
	an adjacency matrix:
	</p>

		<pre><code class="language-java">modelBuilder
	.operation("src=from(A)")
	.operation("dst=to(A)")
	.operation("message{l}=h{l}[src] | h{l}[dst]");</code></pre>

		<p>The model builder parses <code class="language-java">|</code> as the
		horizontal concatenation expression. One may
		also construct a global independent edge feature
		matrix and concatenate that as well.</p>

		<p>Given a constructed message, continue by defining any kind of ad-hoc
		mechanism or neural processing of messages with
		traditional matrix operations (take care to define correct matrix sizes 
		for dense transformations, e.g., twice the number of
		columns as <code class="language-java">H{l}</code> in the previous
		snippet). For any kind of <code class="language-java">LayeredBuilder</code>, 
		don't forget that <code class="language-java">message{l}</code> within 
		operations is needed to obtain a message from the representations 
		<code class="language-java">h{l}</code> that is not accidentally shared with future layers.
		</p>

		<p>Receiver mechanisms need to perform some kind of
		reduction on messages. JGNN implements summation
		reduction, given that this has the same
		theoretical expressive power as maximum-based
		reduction but is easier to backpropagate
		through. Perform this like below. 
		The sum is weighted per the values of
		the adjacency matrix <code class="language-java">A</code>. Thus,
		perform adjacency matrix normalization only if
		you want such weighting to occur.</p>

		<pre><code class="language-java">modelBuilder
	.operation("received{l}=reduce(transformed_message{l}, A)")</code></pre>

		<p>You can finally define layers that transform node
			representations while accounting for received
			messages:</p>

		<pre><code class="language-java">modelBuilder
	.config("2feats{l}", ...)
	.layer("h{l+1}=(h{l} | received{l})@matrix(2feats,dims) + vector(dims)")</code></pre>

		<p>where <code>2feats{l}</code> is configured to a
			matching number of dimensions as the sum of the
			number of columns of <code>h{l}</code> and
			<code>transformed_message{l}</code>.
		</p>

		<p>A common realization of message-passing GNNs is
			via sparse-dense matrix multiplication to
			emulate neighbor attention per
			<code>A.(h<sup>T</sup>h)</code>, where
			<code class="language-java">A</code> is a sparse adjacency matrix,
			<code class="language-java">.</code> is the Hadamard product
			(element-by-element multiplication), and
			<code class="language-java">h</code> is a dense matrix whose rows hold
			respective node representations. JGNN implements
			this operation, and you can include it in
			symbolic definitions with the expression
			<code class="language-java">att(A, h)</code>. Its implementation is
			considerably more lightweight than the
			equivalent message-passing mechanism.
		</p>

		<p>True neighbor attention in the style of gated
			attention networks can be implemented by
			exponentiating all non-zero elements of the
			adjacency matrix and performing row-wise
			normalization per <code class="language-java">L1(nexp(att(A, h)))</code>.</p>
	
	

		<p>As an example of how to define a full GNN with symbolic
			parsing, let's define the well-known APPNP architecture.
			This architecture comprises two dense layers and
			propagates their predictions through the graph structure
			with a fixed-depth approximation of the personalized
			PageRank algorithm.</p>

		<p>To define the architecture, let's consider a
			<code class="language-java">Dataset dataset</code> loaded by the library, for
			which we normalize the adjacency matrix and send
			everything to the GNN builder class. We let the outcome
			of the first two dense layers be remembered as
			<code class="language-java">h{0}</code> (this is <em>not</em>
			<code class="language-java">h0</code>), define a diffusion rate constant
			<code class="language-java">a</code>, and then perform 10 times the
			personalized PageRank diffusion scheme on a graph with
			edge dropout of 0.5. This is all achieved with the same
			<code class="language-java">layer</code> and <code class="language-java">layerRepeat</code> methods
			as neural builders:
		</p>

		<pre><code class="language-java">dataset.graph().setMainDiagonal(1).setToSymmetricNormalization();
long numClasses = dataset.labels().getCols();

ModelBuilder modelBuilder = new FastBuilder(dataset.graph(), dataset.features())
	.config("reg", 0.005)
	.config("hidden", 16)
	.config("classes", numClasses)
	.layer("h{l+1}=relu(h{l}@matrix(features, hidden, reg)+vector(hidden))")
	.layer("h{l+1}=h{l}@matrix(hidden, classes)+vector(classes)")
	.rememberAs("0")
	.constant("a", 0.9)
	.layerRepeat("h{l+1} = a*(dropout(A, 0.5)@h{l})+(1-a)*h{0}", 10)
	.classify();</code></pre>

		<p>GNN classification models can be backpropagated by
			considering a list of node indices and desired
			predictions for those nodes. However, you can also use
			the interfaces discussed in the <a href="tutorials/Learning.md">learning tutorial</a>
			to automate the training process and control it in a
			fixed manner.</p>

		<p>Recall that training requires calling the model's method
			<code class="language-java">.train(optimizer, features, labels, train,
					valid)</code>. The important question is what to
			consider as training inputs and outputs, given that node
			features and the graph are passed to the
			<code class="language-java">FastBuilder</code> constructor.
		</p>

		<p>The answer is that the (ordered) list of all node
			identifiers <code class="language-java">0,1,2,...</code> constitutes the
			training inputs and the corresponding labels constitute
			the outputs. You can create a slice of identifiers and
			use JGNN to design the training process:</p>

		<pre><code class="language-java">Slice nodes = dataset.samples().getSlice().shuffle(100);  // or nodes = new Slice(0, numNodes).shuffle(100);
Model model = modelBuilder()
.getModel()
.init(...)
.train(trainer,
   nodes.samplesAsFeatures(), 
   dataset.labels(), 
   nodes.range(0, trainSplit), 
   nodes.range(trainSplit, validationSplit));</code></pre>

		<p>In the above snippet, the label matrix can have zeros for
			the nodes not used for training. If only the first nodes
			have known labels, the label matrix may also have fewer
			rows.</p>
	</section>


	<section id="create-data">
		<h1>5. Create Data</h1>
		<p>JGNN provides automatically downloaded dataset classes that automatically
		download their data and are ready to test with.
		In practice, though, you will want to use your own data. We now cover how to 
		manually fill in data, as well as which operations are provided to manipulate
		those data. Read more of these operations to learn how to process neural
		inputs, post-process learning outcomes, create custom
		parameters, contribute to the library with more components,
		or make derivative works based on native Java vector and
		matrix arithmetics. Covered operations are also performed
		under-the-hood by Neuralang, so there is no need to dive in
		too deep in the first read of this guidebook.
		</p>
		
		<h3 id="fill-in-data">5.1. Fill in data</h3>
		<p>In the simplest case, both the number of nodes or data samples, and
		the number of feature dimensions are known beforehand. If so, create
		dense feature matrices with the following code. This uses the 
		minimum memory necessary to construct the feature matrix. If
		features are dense (do not have a lot of zeros),
		consider using the <code class="language-java">DenseMatrix</code> class
		instead of initializing a sparse matrix, like below.</p>

			<pre><code class="language-java">Matrix features = new SparseMatrix(numNodes, numFeatures);
for(long nodeId=0; nodeId&lt;numNodes; nodeId++)
	for(long featureId=0; featureId&lt;numFeatures; featureId++)
		features.put(nodeId, featureId, 1);</code></pre>
		</section>

		<p>Sometimes, it is easier to read node or sample features
		line-by-line, for instance, when reading a <em>.csv</em>
		file. In this case, store each line as a separate tensor, 
		so that list of row tensors can then be converted into a
		feature matrix like in the example below.</p>

			<pre><code class="language-java">ArrayList<Tensor> rows = new ArrayList<Tensor>();
try(BufferedReader reader = new BufferedReader(new FileReader(file))){
	String line = reader.readLine();
	while (line != null) {
		String[] cols = line.split(",");
		Tensor features = new SparseTensor(cols.length);
		for(int col=0;col&lt;cols.length;col++)
			features.put(col, Double.parseDouble(cols[col]));
		rows.add(features);
		line = reader.readLine();
	}
}
Matrix features = new WrapRows(rows).toSparse();</code></pre>

			
		<p>Creating adjacency matrices is similar to creating
		preallocated feature matrices. When in doubt, use the <b>sparse</b>
		format for adjacency matrices, as their allocated memory scales
		with the square of the number of nodes. Note that many GNNs
		consider bidirectional (i.e., non-directed) edges, in which case	
		both should be added together. Use the following snippet as a
		template for constructing adjacency matrices. Recall that JGNN
		follows a function chain notation; in this case, each modification
		returns the <code class="language-java">matrix</code> instance for further modifications.
		Don't forget to normalize or apply the
		renormalization trick (self-edges) on matrices if these
		are needed by your architecture, for instance by calling
		<code class="language-java">adjacency.setMainDiagonal(1).setToSymmetricNormalization();</code>
		after matrix construction.</p>

		<pre><code class="language-java">Matrix adjacency = new SparseMatrix(numNodes, numNodes);
for(Entry&lt;Long, Long&gt; edge : edges)
	matrix
		.put(edge.getKey(), edge.getValue(), 1)
		.put(edge.getValue(), edge.getKey(), 1);</code></pre>

		<h3 id="identifiers">5.2. Identifiers</h3>
		<p>The above snippets all make use of numerical node identifiers. To
		manage these, JGNN provides an <code class="language-java">IdConverter</code> class. 
		You can convert hashable
		objects (e.g., Strings) to identifiers by calling
		<code  class="language-java">IdConverter.getOrCreateId(object)</code>. The same
		functionality is also helpful for one-hot encoding of
		class labels. To search only for previously
		registered identifiers, use
		<code class="language-java">IdConverter.get(object)</code>.
		For example, construct a label matrix of one-hot
		encodings for training data:</p>

		<pre><code class="language-java">IdConverter nodeIds = new IdConverter();
IdConverter classIds = new IdConverter();
for(Entry<String, String> entry : nodeLabels) {
	nodeids.getOrCreateId(entry.getKey());
	classIds.getOrCreateId(entry.getValue());
}
Matrix labels = new SparseMatrix(nodeIds.size(), classIds.size());
for(Entry<String, String> entry : nodeLabels) 
labels.put(nodeids.get(entry.getKey()), classIds.get(entry.getValue()), 1);</code></pre>

		<p>Reverse-search the <code class="language-java">IdConverter</code> to obtain the original object
		of predictions using <code class="language-java">IdConverter.get(String)</code>. For example:
		</p>

		<pre><code class="language-java">long nodeId = nodeIds.get("nodeName");
Tensor prediction = labels.accessRow(nodeId);
long predictedClassId = prediction.argmax();
System.out.println(classIds.get(predictedClassId));</code></pre>
	</section>
	
			<h3 id="tensor-operations">5.3. Tensor Operations</h3>
			<p>Tensor operations are performed element-by-element and
				can be split into the following categories:</p>

			<ul>
				<li><strong>arithmetic</strong> - combine the values of
					two tensors to create a new one</li>
				<li><strong>in-place arithmetic</strong> - combine the
					values of two tensors to alter the first one</li>
				<li><strong>summary statistics</strong> - output simple
					numeric values</li>
				<li><strong>element access</strong> - manipulation of
					specific values</li>
			</ul>

			<p>In-place arithmetics follow the same naming
			conventions of base arithmetics and begin with a "self"
			prefix for pairwise operations or "setTo" prefix to
			perform operators. First we present commonly used operations 
			applicable to all tensors, whose functionality is inferable from
			their name and argument types.</p>

			<table class="table table-bordered">
				<thead>
					<tr>
						<th>Operation</th>
						<th>Type</th>
						<th>Comments</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><code class="language-java">Tensor copy()</code></td>
						<td>arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor zeroCopy()</code></td>
						<td>arithmetic</td>
						<td>Zero copies share the same type with the
							tensor and comprise only zeros.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor add(Tensor)</code></td>
						<td>arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor substract(Tensor)</code></td>
						<td>arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor multiply(Tensor)</code></td>
						<td>arithmetic</td>
						<td>Multiplication is performed
							element-by-element.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor multiply(double)</code></td>
						<td>arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor normalized()</code></td>
						<td>arithmetic</td>
						<td>Division with L2 norm (if non-zero).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor toProbability()</code></td>
						<td>arithmetic</td>
						<td>Division with the sum (if non-zero).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setToZero()</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor selfAdd(Tensor)</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor selfSubtract(Tensor)</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setMultiply(Tensor)</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor selfMultiply(double)</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setToRandom()</code></td>
						<td>in-place arithmetic</td>
						<td>element selected from uniform distribution
							in the range [0,1]</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setToOnes()</code></td>
						<td>in-place arithmetic</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setToNormalized()</code></td>
						<td>in-place arithmetic</td>
						<td>Division with L2 norm (if non-zero).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setToProbability()</code></td>
						<td>in-place arithmetic</td>
						<td>Division with the sum (if non-zero).</td>
					</tr>
					<tr>
						<td><code class="language-java">double dot(Tensor)</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">double norm()</code></td>
						<td>summary statistics</td>
						<td>The L2 norm.</td>
					</tr>
					<tr>
						<td><code class="language-java">double sum()</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">double max()</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">double min()</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">long argmax()</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">long argmin()</code></td>
						<td>summary statistics</td>
					</tr>
					<tr>
						<td><code class="language-java">double toDouble()</code></td>
						<td>summary statistics</td>
						<td>Converts tensor with exactly one element to
							a double (throws exception if more
							elements).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor set(long position, double value)</code></td>
						<td>element access</td>
						<td>Is in-place.</td>
					</tr>
					<tr>
						<td><code class="language-java">double get(long position)</code></td>
						<td>element access</td>
					</tr>
					<tr>
						<td><code class="language-java">Iterator<long> getNonZeroElements()</code></td>
						<td>element access</td>
						<td>Traverses all elements for dense
							tensors, but skips zero elements for
							sparse tensors. (Guarantee: there is no
							non-zero element not traversed.) Returns
							element <i>positions</i>. To write code that 
							accommodates both dense and
							sparse tensors, ensure that iterating over indices
							elements is performed with this method.</td>
					</tr>
					<tr>
						<td><code class="language-java">String describe()</code></td>
						<td>summary statistics</td>
						<td>Description of type and dimensions.</td>
					</tr>
				</tbody>
			</table>

			<p>Prefer in-place arithmetic operations when
			transforming tensor values or for intermediate
			calculation steps, as these do not allocate new
			memory. For example, the following code can be
			used for creating and normalizing a tensor of
			ones without using additional memory:</p>

			<pre><code class="language-java">Tensor normalized = new DenseTensor(10).setToOnes().setToNormalized();</code></pre>

			<p>Initialize a dense or sparse tensor with their number
			of elements. If there are many zero elements expected,
			prefer using a sparse tensor.</p>

			<pre><code class="language-java">long size = ...;
Tensor denseTensor = new mklab.JGNN.tensor.DenseTensor(size);
Tensor sparseTensor = new mklab.JGNN.tensor.SparseTensor(size);</code></pre>

			<p>For example, one-hot encodings for classification
				problems can be generated with the following
				code. This creates a dense tensor with
				<code class="language-java">numClasses</code> elements and puts at
				element <code class="language-java">classId</code> the value 1:
			</p>

			<pre><code class="language-java">int classId = ...;
int numClasses = ...;
Tensor oneHotEncoding = new mklab.JGNN.tensor.DenseTensor(numClasses).set(classId, 1);</code></pre>

			<p>Dense tensors are serialized with their
				<code class="language-java">String toString()</code> method and can be
				deserialized into new tensors with the
				constructor
				<code class="language-java">mklab.JGNN.tensor.DenseTensor(String)</code>.
			</p>
			<p>The <code class="language-java">Matrix</code> class extends the concept
				of tensors with additional operations. Under the
				hood, matrices linearly store elements and use
				computations to transform the (row, col)
				position of their elements to respective
				positions. The outcome of some methods inherited
				from tensors may need to be typecast back into a
				matrix (e.g., for all in-place operations).</p>

			<table class="table table-bordered">
				<thead>
					<tr>
						<th>Operation</th>
						<th>Type</th>
						<th>Comments</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><code class="language-java">Matrix onesMask()</code></td>
						<td>arithmetic</td>
						<td>Copy of a matrix with elements set
							to one.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix transposed()</code></td>
						<td>arithmetic</td>
						<td>There is no method for in-place
							transposition.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix asTransposed()</code></td>
						<td>arithmetic</td>
						<td>Shares data with the original.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor getRow(long)</code></td>
						<td>arithmetic</td>
						<td>Shares data with the original.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor getCol(long)</code></td>
						<td>arithmetic</td>
						<td>Shares data with the original.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor transform(Tensor x)</code></td>
						<td>arithmetic</td>
						<td>Outputs a dense tensor that holds
							the linear transformation of the
							given tensor (using it as a column
							vector) by multiplying it with the
							matrix.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix matmul(Matrix with)</code></td>
						<td>arithmetic</td>
						<td>Outputs the matrix multiplication
							**this \* with**. There is no
							in-place matrix multiplication.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix matmul(Matrix with, boolean transposeSelf, boolean transposeWith)</code></td>
						<td>arithmetic</td>
						<td>Does not perform memory allocation
							to compute transpositions.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix external(Tensor horizontal, Tensor vertical)</code></td>
						<td>static method</td>
						<td>External product of two tensors. Is a dense matrix.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix symmetricNormalization()</code></td>
						<td>in-place arithmetic</td>
						<td>The symmetrically normalized matrix.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix setToSymmetricNormalization()</code></td>
						<td>in-place arithmetic</td>
						<td>The symmetrically normalized matrix.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix setMainDiagonal(double value)</code></td>
						<td>in-place arithmetic</td>
						<td>Sets diagonal elements.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix setDiagonal(long diagonal, double value)</code></td>
						<td>in-place arithmetic</td>
						<td>Sets diagonal elements.</td>
					</tr>
					<tr>
						<td><code class="language-java">Matrix put(long row, long col, double value)</code></td>
						<td>element access</td>
						<td>Is in-place.</td>
					</tr>
					<tr>
						<td><code class="language-java">Iterable&lt;entry&lt;long, Long&gt;&gt; getNonZeroEntries()</code></td>
						<td>element access</td>
						<td>Similar to getNonZeroElements() but iterates through (row, col)pairs.</td>
					</tr>
				</tbody>
			</table>
			
	
	<section id="training">
		<h1>6. Training</h1>
		<p>Most neural network architectures are designed with the idea
		of learning to classify nodes or samples. However, GNNs also
		provide the capability to classify entire graphs based on
		their structure. To define architectures for graph classification, we use
		the generic <code class="language-java">LayeredBuilder</code> class. The main
		difference compared to traditional neural networks is
		that architecture inputs do not all exhibit the same
		size (e.g., some graphs may have more nodes than others)
		and therefore cannot be organized into tensors of common
		dimensions. Instead, assume that training data are stored in the
		following lists:</p>

			<pre><code class="language-java">ArrayList<Matrix> adjacencyMatrices = new ArrayList<Matrix>();
ArrayList<Matrix> nodeFeatures = new ArrayList<Matrix>();
ArrayList<Tensor> graphLabels = new ArrayList<Tensor>();</code></pre>

		<p>The <code class="language-java">LayeredBuilder</code> class introduces the
		input variable <code class="language-java">h<sub>0</sub></code> for sample
		features. We can use it to pass node features to the
		architecture, so we only need to add a second input
		storing the (sparse) adjacency matrix:</p>

		<pre><code class="language-java">.var("A")</code></pre>

		<p>We can then proceed to define a GNN architecture, for
		instance as explained in previous tutorials. This time,
		though, we aim to classify entire graphs rather than
		individual nodes. For this reason, we need to pool top
		layer node representations, for instance by averaging
		them across all nodes:</p>

		<pre><code class="language-java">.layer("h{l+1}=softmax(mean(h{l}, dim: 'row'))")</code></pre>

		<p>Finally, we need to set up the top layer as the built
		model's output per: <code class="language-java">.out("h{l}")</code>
		An example architecture following these principles follows:</p>

                <pre><code class="language-java">ModelBuilder builder = new LayeredBuilder()
    .var("A")  
    .config("features", nodeLabelIds.size())
    .config("classes", graphLabelIds.size())
    .config("hidden", 16)
    .layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden)))") 
    .layer("h{l+1}=relu(A@(h{l}@matrix(hidden, classes)))") 
    .layer("h{l+1}=softmax(mean(h{l}, dim: 'row'))")
    .out("h{l}");</code></pre>

			<p>For the time being, training architectures like the above
				on prepared data requires manually calling the
				backpropagation for each epoch and each graph in the
				training batch. To do this, first retrieve the model and
				initialize its parameters:</p>

			<pre><code class="language-java">Model model = builder.getModel().init(new XavierNormal());</code></pre>

			<p>Next, define a loss function and set up a batch
				optimization strategy wrapping any base optimizer and
				accumulating parameter updates until
				<code class="language-java">BatchOptimizer.updateAll()</code> is called later
				on:
			</p>

			<pre><code class="language-java">Loss loss = new CategoricalCrossEntropy();
BatchOptimizer optimizer = new BatchOptimizer(new Adam(0.01));</code></pre>

			<p>Finally, training can be conducted by iterating through
				epochs and training samples and appropriately calling
				the <code class="language-java">Model.train</code> for combinations of node
				features and graph adjacency matrix inputs and graph
				label outputs. At the end of each batch (e.g., each
				epoch), don't forget to call the
				<code class="language-java">optimizer.updateAll()</code> method to apply the
				accumulated gradients. This process can be realized with
				the following code:
			</p>

			<pre><code class="language-java">for(int epoch=0; epoch&lt;300; epoch++) {
	for(int graphId=0; graphId&lt;graphLabels.size(); graphId++) {
		 Matrix adjacency = adjacencyMatrices.get(graphId);
		 Matrix features = nodeFeatures.get(graphId);
		 Tensor label = graphLabels.get(graphId);
		 model.train(loss, optimizer, 
			  Arrays.asList(features, adjacency), 
			  Arrays.asList(label));
	}
	optimizer.updateAll();
}</code></pre>

            <section id="sort-pooling">
                <h3>Sort Pooling</h3>
                <p>Up to now, the example code performs a naive mean pooling
                    across all graph node features. However, this can prove
                    insufficient for the top layers, and more sophisticated
                    pooling mechanisms can be deployed to let GNNs
                    differentiate between the structural positioning of
                    nodes to be pooled.</p>

                <p>One computationally light approach to pooling, which JGNN
                    implements, is sorting nodes based on learned features
                    before concatenating their features into one vector for
                    each graph. This process is further simplified by
                    keeping the top <em>reduced</em> number of nodes to
                    concatenate their features, where the order is
                    determined by an arbitrarily selected feature (in our
                    implementation: the last one, with the previous feature
                    being used to break ties, and so on).</p>

                <p>The idea is that the selected feature determines
                    <em>important</em> nodes whose information can be
                    adopted by others. To apply the above operations, JGNN
                    provides independent operations to sort nodes, gather
                    node latent representations, and reshape matrices into
                    row or column tensors with learnable transformations to
                    class outputs. These components are demonstrated in the
                    following code snippet:
                </p>

                <pre><code class="language-java">long reduced = 5;  // input graphs need to have at least that many nodes
long hidden = 8;  // many latent dims reduce speed without GPU parallelization

ModelBuilder builder = new LayeredBuilder()        
    .var("A")  
    .config("features", 1)
    .config("classes", 2)
    .config("reduced", reduced)
    .config("hidden", hidden)
    .layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden))+vector(hidden))")
    .layer("h{l+1}=relu(A@(h{l}@matrix(hidden, hidden))+vector(hidden))")
    .concat(2) // concatenates the outputs of the last 2 layers
    .config("hiddenReduced", hidden*2*reduced)  // 2* due to concatenation
    .operation("z{l}=sort(h{l}, reduced)")
    .layer("h{l+1}=reshape(h{l}[z{l}], 1, hiddenReduced)")
    .layer("h{l+1}=h{l}@matrix(hiddenReduced, classes)")
    .layer("h{l+1}=softmax(h{l}, dim: 'row')")
    .out("h{l}");</code></pre>
            </section>

            <section id="parallelized-training">
                <h3>Parallelized Training</h3>
                <p>To speed up graph classification, you can use JGNN's
                    parallelization capabilities to calculate gradients
                    across multiple threads. Parallelization for node
                    classification holds little meaning, as the same
                    propagation mechanism needs to be run on the same graph
                    in parallel. However, this process yields substantial
                    speedup for the <em>graph</em> classification
                    problem.</p>

                <p>Parallelization can make use of JGNN's thread pooling to
                    perform gradients, wait for the conclusion of submitted
                    tasks, and then apply all gradient updates. This is
                    achieved by declaring a batch optimizer to gather all
                    the gradients. The entire process is detailed in the
                    following example:</p>

                <pre><code class="language-java">for(int epoch=0; epoch&lt;500; epoch++) {
    // gradient update
    for(int graphId=0; graphId&lt;dtrain.adjucency.size(); graphId++) {
        int graphIdentifier = graphId;
        ThreadPool.getInstance().submit(new Runnable() {
            @Override
            public void run() {
                Matrix adjacency = dtrain.adjucency.get(graphIdentifier);
                Matrix features= dtrain.features.get(graphIdentifier);
                Tensor graphLabel = dtrain.labels.get(graphIdentifier).asRow();  
                model.train(loss, optimizer, 
		            Arrays.asList(features, adjacency), 
		            Arrays.asList(graphLabel));
            }
        });
    }
    ThreadPool.getInstance().waitForConclusion();  // waits for all gradients to finish calculating
    optimizer.updateAll();

    double acc = 0.0;
    for(int graphId=0; graphId&lt;dtest.adjucency.size(); graphId++) {
        Matrix adjacency = dtest.adjucency.get(graphId);
        Matrix features= dtest.features.get(graphId);
        Tensor graphLabel = dtest.labels.get(graphId);
        if(model.predict(Arrays.asList(features, adjacency)).get(0).argmax()==graphLabel.argmax())
           acc += 1;
        System.out.println("iter = " + epoch + "  " + acc/dtest.adjucency.size());
    }
}</code></pre>
            </section>
        </section>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-rust.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>