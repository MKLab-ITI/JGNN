<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JGNN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-vs.min.css"
        integrity="sha512-Jn4HzkCnzA7Bc+lbSQHAMeen0EhSTy71o9yJbXZtQx9VvozKVBV/2zfR3VyuDFIxGvHgbOMMNvb80l+jxFBC1Q=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        body {
            display: flex;
            overflow-x: hidden; /* Prevent horizontal scrolling */
        }
		
		.table td, .table th {
		  min-width: 100px; /* Adjust the value as needed */
		}

        #sidebar {
            position: fixed;
            top: 0;
            left: 0;
            height: 100%;
            width: 190px;
            background-color: #f8f9fa;
            padding-top: 20px;
            padding-left: 5px;
            border-right: 1px solid #ddd;
        }

        .sidebar-title {
            text-align: center;
        }

        #content {
            margin-left: 200px;
            padding: 20px;
            flex-grow: 1;
            overflow-wrap: break-word;
            word-wrap: break-word;
            word-break: break-word;
            width: 100%; /* Ensures content takes full width */
        }

        section {
            margin-top: 90px;
        }

        .sidebar-link {
            color: #000;
            text-decoration: none;
            padding: 10px;
            display: block;
        }

        .sidebar-link.active {
            color: #fff;
            background-color: #007bff;
            border-radius: 4px;
        }

        @media (max-width: 767.98px) {
            #sidebar {
                display: none;
            }

            .navbar {
                position: fixed;
                display: block;
                color: #fff;
                background-color: #007bff;
                border-radius: 4px;
            }

            #content {
                margin-left: 0;
                padding: 20px;
                flex-grow: 1;
                overflow-wrap: break-word;
                word-wrap: break-word;
                word-break: break-word;
            }
        }

        @media (min-width: 768px) {
            .navbar {
                display: none;
            }
        }
    </style>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            const sidebarLinks = document.querySelectorAll('.sidebar-link');

            function removeActiveClasses() {
                sidebarLinks.forEach(link => link.classList.remove('active'));
            }

            function addActiveClass(link) {
                removeActiveClasses();
                link.classList.add('active');
            }

            const options = {
                root: null,
                rootMargin: '0px',
                threshold: 0.2
            };

            const observerCallback = (entries, observer) => {
                entries.forEach(entry => {
                    const id = entry.target.getAttribute('id');
                    const link = document.querySelector(`.sidebar-link[href="#${id}"]`);

                    if (entry.isIntersecting) {
                        addActiveClass(link);
                    }
                });
            };

            const observer = new IntersectionObserver(observerCallback, options);

            document.querySelectorAll('section').forEach(section => {
                observer.observe(section);
            });
        });
    </script>
</head>

<body>
    <nav id="sidebar">
        <ul class="nav flex-column">
            <li class="nav-item sidebar-title"><h5>JGNN</h5></li>
            <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#modelbuilder"
                    style="color: #777777;">3.1. ModelBuilder</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#fastbuilder"
                    style="color: #777777;">3.2. FastBuilder</a></li>
			<li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#neuralang"
					style="color: #777777;">3.3. Neuralang</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#debugging"
                    style="color: #777777;">3.4. Debugging</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#training">4. Training</a></li>
			<li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#create-data"
					style="color: #777777;">4.1. Create data</a></li>
            <li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#node-classification"
                    style="color: #777777;">4.2. Node classification</a></li>
				<li class="nav-item ps-md-3 text-secondary"><a class="sidebar-link small p-1 subsection" href="#graph-classification"
						style="color: #777777;">4.3. Graph classification</a></li>
        </ul>
    </nav>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <div class="container-fluid">
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#training">4. Training</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <div id="content">
		<h1 style="text-align: center;">JGNN</h1>
		<p>Graph Neural Networks (GNNs) are getting more and more popular as a machine learning paradigm,
		for example to make predictions 
		based on relational information, or to perform inference on small datasets. JGNN is a library that
		provides cross-platform implementations of this paradigm without the need for dedicated 
		hardware or firmware; create highly portable models that fit and are trained in
		a few megabytes of memory. While reading this guidebook, keep in mind that this is not a library 
		for running computationally intensive stuff; it has no GPU support and we do not plan to 
		add any (unless such support becomes integrated in the Java virtual machine). So, while source code is
		highly optimized and complex architectures are supported, 
		running them fastly on graphs with many nodes may require compromises in the number of learned 
		parameters or running time.</p>
		
		<p>This guidebook is organized into four sections that focus on 
		practical use. After this brief introduction and
		instructions for how to set things up, <a href="#quickstart">section 2</a> 
		gives a taste of what using the library looks like. Then,
		<a href="#gnn-builders">section 3</a> describes the
		library's builder patter for constructing GNN models. Model construction
		includes symbolic expression parsing for machine learning operations,
		which drastically simplifies coding. Parsed expressions follow the Neuralang 
		scripting language for model definitions. Finally, <a href="#training">section 4</a> describes
		interfaces for training on automatically generated or customized data and testing. It also takes a deep dive
		into obtaining raw model predictions, and using them in custom training
		and evaluation schemes.
		</p>
		
		<p>
		In addition to the above-described material, JGNN's full programmatic
		interface is provided as <a href="https://mklab-iti.github.io/JGNN/javadoc/" target="_blank">Javadoc</a>,
		and domain-specific examples reside in the project's <a href="https://github.com/MKLab-ITI/JGNN" target="_blank">GitHub repository</a>.
		</p>
		
	<section id="setup">
	<h1>1. Setup</h1>
	<p>The simplest way to set up JGNN is to download it as a JAR package from
	<a href="https://github.com/MKLab-ITI/JGNN/releases">releases</a> 
	and add it your Java project's build path. Those working with Maven 
	or Gradle can instead add JGNN's latest nightly release as a dependency from its JitPack 
	distribution. Follow the link below for full instructions.<br>
	<a href="https://jitpack.io/#MKLab-ITI/JGNN"><img src="https://jitpack.io/v/MKLab-ITI/JGNN.svg" alt="download JGNN"></a>
	</p>
	<p>
	For example, the fields in the snippet below may be added in a Maven <em>pom.xml</em> file 
	to work with the latest nightly release.</p>
	<pre><code class="language-xml">&lt;repositories&gt;
	&lt;repository&gt;
		&lt;id&gt;jitpack.io&lt;/id&gt;
		&lt;url&gt;https://jitpack.io&lt;/url&gt;
	&lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
	&lt;dependency&gt;
		&lt;groupId&gt;com.github.MKLab-ITI&lt;/groupId&gt;
		&lt;artifactId&gt;JGNN&lt;/artifactId&gt;
		&lt;version&gt;SNAPSHOT&lt;/version&gt;
	&lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
	</section>
	
	<section id="quickstart">
	<h1>2. Quickstart</h1>

	<p>Here we demonstrate usage of JGNN for node classification. This is an inductive learning
	task that predicts node labels given a graph's structure, node features, and a some already known
	labels. Classifying graphs is also supported, although it is a harder task to explain and set up. 
	GNN architectures for node classification are typically written 
	as message passing mechanisms; they diffuse node representations across edges, where
	node neighbors pick up, aggregate (e.g., average), and transform
	incoming representations to update theirs. Alternatives that boast higher 
	expressive power also exist and are supported, but simple architectures 
	may be just as good or better than complex alternatives in solving 
	practical problems <a href="https://www.mdpi.com/2076-3417/14/11/4533">[Krasanakis et al., 2024]</a>.
	Simple architectures also enjoy reduced resource consumption.</p>
	
	<p>Our demonstration starts by loading the <code class="language-java">Cora</code> dataset from those shipped 
	with the library for out-of-the-box testing. The first time an instance of this dataset is created,
	it downloads its raw data from a web resource and stores them in a local <code class="language-java">downloads/</code>
	folder. The data are then loaded into a sparse graph adjacency matrix, a dense node feature matrix, 
	and a dense node label matrix. Sparse and dense representations are interchangeable in terms of operations,
	with the main difference being that sparse matrices are much more efficient when they contain lots of zeros.
	In the loaded matrices, each row contains the corresponding node's
	neighbors, features, or one-hot encoding of labels. We apply the renormalization trick and
	symmetric normalization on the dataset's adjacency matrix using in-place operations for minimal memory footprint;
	the first of the two makes GNN computations numerically stable by adding self-loops 
	to all nodes, while renormalization is required by spectral-based GNNs, such as 
	the model we implement next.</p>
	
	<pre><code class="language-java">Dataset dataset = new Cora();
dataset.graph().setMainDiagonal(1).setToSymmetricNormalization();</code></pre>
	
	
	<p>We now incrementally create a trainable model using symbolic expressions that resemble math 
	notation. The expressions are part of a scripting language, called Neuralang,
	that is covered in <a href="#neuralang">section 3.3</a>. However, for faster onboarding, stick to
	the <code class="language-java">FastBuilder</code> class for creating models; this ommits some of
	the language's features in favor of providing programmatic shortcuts for boilerplate code. Its constructor
	accepts two arguments <code>A</code> and <code>h0</code>, respectivel holding
	the graph's adjacency matrix and node features. These are internally set as constant symbols that 
	parse expressions can use. Other constants and input variables can be set too, 
	but more on this later. After instantiation, use some
	model builder methods to declare a model's dataflow. Some of these methods parse the aforementioned expressions.
	<ul>
		<li><code>config</code> - Configures hyperparameter values. These can be used in all subsequent function and layer declarations.</li>
		<li><code>function</code> - Declares a Neuralang function, in this case with inputs <code>A</code> and <code>h</code>.</li>
		<li><code>layer</code> - Declares a layer that can use builtin and Neuralang functions. In this, the symbols <code>{l}</code> and <code>{l+1}</code> specifically are replaced by a layer counter.</li>
		<li><code>classify</code> - Adds a softmax layer tailored to classification. This also silently declares an input <code>nodes</code> that represents a list of node indices where the outputs should be computed.</li>
		<li><code>autosize</code> - Automatically sizes matrix and vector dimensions that were originally defnoted with a questionmark <code>?</code>. This method requires some input example, and here we provide a list of node identifiers, which we also make dataless (have only the correct dimensions without allocating memory). This method also checks for integrity errors in the declared architecture, such as computational paths that do not lead to an output.</li>
	</ul>
	JGNN promotes method chains, where the builder's instance is returned by each of 
	its methods to access the next one. Below we use this programming pattern to implement the Graph Convolutional Network (GCN) 
	architecture <a href="https://arxiv.org/abs/1609.02907">[Kipf and Welling, 2017]</a>.
	Details on the symbolic parts of definitions are presented later but, for the time being, we point to
	the <code>matrix</code> and <code>vector</code> Neuralang functions. These inline declarations of learnable parameter for
	given dimensions and regularization. Access the builder's created model via <code>modelBuilder.getModel()</code>.
	</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new FastBuilder(dataset.graph(), dataset.features())
	.config("reg", 0.005)
	.config("classes", numClasses)
	.config("hidden", 64)
	.function("gcnlayer", "(A,h){Adrop = dropout(A, 0.5); return Adrop@(h@matrix(?, hidden, reg))+vector(?);}")
	.layer("h{l+1}=relu(gcnlayer(A, h{l}))")
	.config("hidden", "classes")  // reassigns the output gcnlayer's "hidden" to be the number of "classes"
	.layer("h{l+1}=gcnlayer(A, h{l})")
	.classify()
	.autosize(new EmptyTensor(numSamples));</code></pre>
	
	
	<p>Training epochs for the created model can be implemented
	manually, by passing inputs, obtaining outputs, computing losses, and triggering backpropagation
	on an optimizer. As these steps may be complicated, JGNN automates common 
	training patterns with a <code>ModelTraining</code> class. Instances of this class
	accept a method chain notation to set their parameters, like the number of epochs, patience 
	for early stopping, the employed optimizer, and loss functions. An example is presented below,
	where <code>Adam</code> optimization with learning rate <i>0.01</i> is performed, and a verbose 
	variation of a validation loss prints the progress. To run a full training process,
	pass the defined strategy to a model alongside input data, corresponding output data, as well
	as training and validation slices.</p>
	<p>In the example, a parameter initializer is applied on the model before training is conducted.
	This is a cold start scenario, as opposed to a warm start that continues training already
	trained parameters.
	Selecting an initializer is not part of training strategies 
	to signify its model-dependent nature; dense layers should maintain the expected 
	input variances in the output before the first epoch, and therefore the initializer depends
	on the type of activation functions. Moreover,
	the graph's adjacency matrix and node features are already declared as constants by the 
	<code>FastBuilder</code> constructor, as node classification takes place on the same graph
	with fully known node features. Architecture anputs are the node identifiers, which in the 
	<code>classify</code> method above are used to gather
	the predictions on respective nodes, and desired outputs are the corresponding labels from
	the dataset. Labels that are not known still need to have some value; as a convention when working
	with your own data, leave the one-hot label encoding of test nodes as zeroes. Doing so in our
	present example would not affect the outcome either. 
	The last two training arguments of the <code>train</code> method
	then accept training and validation data slices. Slices are effectively lists of integer entries 
	pointing to rows of inputs and outputs - find more later.
	</p>
	<pre><code class="language-java">ModelTraining trainer = new ModelTraining()
	.setOptimizer(new Adam(0.01))
	.setEpochs(3000)
	.setPatience(100)
	.setLoss(new CategoricalCrossEntropy())
	.setValidationLoss(new VerboseLoss(new Accuracy()).setInterval(10));  // print validation every 10 epochs
	
Slice nodes = dataset.samples().getSlice().shuffle(); // a permutation of node identifiers
Matrix inputData = Tensor.fromRange(nodes.size()).asColumn(); // each node has its identifier as an input
Model model = modelBuilder.getModel()
		.init(new XavierNormal())
		.train(trainer, 
				inputData,
				dataset.labels(), 
				nodes.range(0, 0.6),  // training slice
				nodes.range(0.6, 0.8)  // validation slice
				);</code></pre>
	
	<p>Trained models and their generating builders can be saved and loaded. The next snippet demonstrates
	how raw predictions can be made too. During this process,
	some matrix manipulation operations are employed to obtain transparent access to parts of input and output data
	of the dataset.
	</p>

	<pre><code class="language-java">modelBuilder.save(Paths.get("gcn_cora.jgnn")); // needs a Path as an input
Model loadedModel = ModelBuilder.load(Paths.get("gcn_cora.jgnn")).getModel(); // loading creates an intermediate modelbuilder

Matrix output = loadedModel.predict(Tensor.fromRange(0, nodes.size()).asColumn()).get(0).cast(Matrix.class);
double acc = 0;
for(Long node : nodes.range(0.8, 1)) {
	Matrix nodeLabels = dataset.labels().accessRow(node).asRow();
	Tensor nodeOutput = output.accessRow(node).asRow();
	acc += nodeOutput.argmax()==nodeLabels.argmax()?1:0;
}
System.out.println("Acc\t "+acc/nodes.range(0.8, 1).size());</code></pre>
	
    </section>
		

	<section id="gnn-builders">
	<h1>3. GNN Builders</h1>
	<p>We already touched on the subject of model builders in the previous section,
	where one of them was used to create a model. There exist different kinds of
	builders that offer different conveniences.
	<ul>
		<li><b>GNNBuilder</b> - Parses strings of simple Neuralang expressions.</li>
		<li><b>FastBuilder</b> - Extends the <code>GNNBuilder</code> class with methods that inject 
		boilerplate code for the inputs, outputs, and layers of node classification tasks.
		Prefer this builder of your want to keep track 
		of the whole model definition in one place within Java code.</li>
		<li><b>Neuralang</b> - Extends the <code>GNNBuilder</code> class so that it can parse all aspects 
		of the Neuralang language, such as functional declarations of machine learning modules,
		where parts of function signatures manage configuration hyperparameters.
		Use this builder to maintain model definitions in one place (e.g., packed in one string
		variable, or in one file) and avoid weaving symbolic expressions in Java code.</li>
	</ul>
	In this section we cover these three builder classes and summarize debugging mechanisms that
	check the integrity of constructed models, visualize their data flow, and monitor specific
	data at runtime.</p>
	
	<h3 id="modelbuilder">3.1. ModelBuilder</h3>
	<p>This is the base model builder class; it offers a wide breadth of functionalities that other builders extend. 
	Before looking at how to use it, though, we need to see what JGNN models look like under the hood. 
	Models are collections of <code  class="language-java">NNOperation</code> instances, each representing a numerical computation with
	specified inputs and outputs of
	JGNN's <code>Tensor</code> type. Tensors will be covered later; for now, it suffices to think of them as
	numerical vectors, which are sometimes endowed with matrix dimensions. This guidebook does not list operation classes, as they are rarely used directly and can be found the Javadoc, namely 
	<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/inputs/package-summary.html" target="_blank">nn.inputs</a>,
	<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/activations/package-summary.html" target="_blank">nn.activations</a>,
	and 
	<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/pooling/package-summary.html" target="_blank">nn.pooling</a>.
	Create models in pure Java like below. The example computes the expression 
	<code class="language-rust">y=log(2*x+1)</code> without any trainable parameters.
	After defining models, run them with the method <code class="language-java">Tensor Model.predict(Tensor...)</code>.	
	This takes as input one or more comma-separated tensors that match the model's
	inputs (in the same order) and computes a list of output tensors. If inputs are dynamically created, 
	an overloaded version of the same method supports an array list of input tensors
	<code class="language-java">Tensor Model.predict(ArrayList&lt;Tensor&gt;)</code>.
	The snippet below includes a prediction for an input that consists of one tensor of one element.
	</p>
	
	<pre><code class="language-java">Variable x = new Variable();
Constant c1 = new Constant(Tensor.fromDouble(1)); // holds the constant "1"
Constant c2 = new Constant(Tensor.fromDouble(2)); // holds the constant "2"
NNOperation mult = new Multiply()
	.addInput(x)
	.addInput(c2);
NNOperation add = new Add()
	.addInput(mult)
	.addInput(c1);
NNOperation y = new Log()
	.addInput(add);
Model model = new Model()
	.addInput(x)
	.addOutput(y);
System.out.println(model.predict(Tensor.fromDouble(2)));
	</code></pre>
	
	<p>Judging by the fact that several lines of code are needed to declare even simple expressions,
	pure Java code for creating full models tends to be cumbersome to read and maintain - hence the need for 
	builders that construct the models from concise symbolic expressions. Let us recreate the above example
	with the <code class="language-java">ModelBuilder</code> class. 
	After instantiating the builder, use a method chain to declare an input variable
	with the <code class="language-java">.var(String)</code> method, parse an expression with the 
	<code class="language-java">.operation(String)</code> method, and finally declare which symbol holds
	outputs with the <code class="language-java">.out(String)</code> method.
	The first and last of these methods can be called multiple times
	to declare several inputs and outputs. Inputs need to be only one symbol, but a whole expression
	for evaluation can be declared in outputs.
	</p>
	<p>
	The operation parses string expressions that are typically structured 
	as assignments to symbols; the right-hand side of assignments accepts several operators and functions that 
	are listed in the next table. Models allow multiple operations too, which are parsed through either multiple 
	method calls or by being separated with a semicolon <code>;</code> within larger string expressions.
	All methods need to use previously declared symbols. For example, parsing <code class="language-java">.out("symbol")</code> 
	throws an exception if no operation previously assigned to the symbol or declared it as an input. For logic
	safety, <b>symbols cannot be overwritten or set to updated values outside of Neuralang functions</b>.
	Finally, the base model builder
	class supports a roundabout declaration of Neuralang functions with expressions like this snippet taken from the Quickstart
	section:
	<code  class="language-java">.function("gcnlayer", "(A,h){return A@(h@matrix(?, hidden, reg))+vector(?);}")</code>.
	In this, the first method argument is the declared function symbol's name, and the second should necessarily have the arguments enclosed in
	a parenthesis and the function's body enclosed in brackets. Learn more about Neuralang functions in 
	<a href="#neuralang">section 3.3</a>.
	</p>
	
	<pre><code class="language-java">ModelBuilder modelBuilder = new ModelBuilder()
	.var("x")
	.operation("y = log(2*x+1)")
	.out("y");
System.out.println(model.predict(Tensor.fromDouble(2)));
</code></pre>
	
	<p>Model definitions have so far been too simple to be employed in practice;
	we need trainable parameters, which are created inline with the <code>matrix</code>
	and <code>vector</code> functions. There is also an equivalent Java
	method <code class="language-java">ModelBuilder.param(String, Tensor)</code> that assigns an initialized Tensor
	to a variable name, but its usage is discouraged to keep model definitions simple. 
	Additionally, there may be constants and configuration hyperparameters. Of these, constants reflect 
	untrainable tensors and set with  <code class="language-java">ModelBuilder.const(String, Tensor)</code>,
	whereas configuration hyperparameters are numerical values used by the parser and 
	set with <code class="language-java">ModelBuilder.config(String, double)</code>, or 
	<code class="language-java">ModelBuilder.config(String, String)</code> if the second argument value
	should be copied from another configuration.
	Both numbers in the last snippet's symbolic definition are internally parsed into constants.
	On the other hand, hyperparameters can be used as arguments to dimension sizes and regularization.
	Retrieve previously set hyperparameters though <code class="language-java">double ModelBuilder.getConfig(String)</code> 
	or <code class="language-java">double ModelBuilder.getConfigOrDefault(String, double)</code>  
	to replace the error with a default value if the configuration is not found. The usefulness of retrieving
	configurations will become apparent later on.
	</p>
	
	<p>
	Next is a table of available operations that you can use in expressions. Standard
	priority rules for priority and parentheses apply.
	Prefer using configuration hyperparameters 
	to set matrix and vector creation, as these transfer their names to respective dimensions for error 
	checking - more on this in <a href="#debugging">section 3.4</a>.
	</p>

    <table class="table" border=" 1">
            <tr>
                <th>Symbol</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><code>x = expr</code></td>
                <td>Operator</td>
                <td>Assign to variable <code>x</code> the outcome of executing expression <code>expr</code>. This expression does not evaluate to anything.</td>
            </tr>
            <tr>
                <td><code>x + y</code></td>
                <td>Operator</td>
                <td>Element-by-element addition.</td>
            </tr>
            <tr>
                <td><code>x * y</code></td>
                <td>Operator</td>
                <td>Element-by-element multiplication.</td>
            </tr>
            <tr>
                <td><code>x - y</code></td>
                <td>Operator</td>
                <td>Element-by-element subtraction.</td>
            </tr>
            <tr>
                <td><code>x @ y</code></td>
                <td>Operator</td>
                <td>Matrix multiplication.</td>
            </tr>
            <tr>
                <td><code>x | y</code></td>
                <td>Operator</td>
                <td>Row-wise concatenation of <code>x</code> and <code>y</code>.</td>
            </tr>
            <tr>
                <td><code>x [y]</code></td>
                <td>Operator</td>
                <td>Gathers the rows of <code>x</code> with indexes <code>y</code>. Indexes are still tensors, whose elements are cast to integers during this operation.</td>
            </tr>
            <tr>
                <td><code>transpose(A)</code></td>
                <td>Function</td>
                <td>Transposes matrix <code>A</code>.</td>
            </tr>
            <tr>
                <td><code>log(x)</code></td>
                <td>Function</td>
                <td>Applies a logarithm on each element of tensor <code>x</code>.</td>
            </tr>
            <tr>
                <td><code>exp(x)</code></td>
                <td>Function</td>
                <td>Exponentiates each element of tensor <code>x</code>.</td>
            </tr>
            <tr>
                <td><code>nexp(x)</code></td>
                <td>Function</td>
                <td>Exponentiates each non-zero element of tensor <code>x</code>. Typically used for neighbor attention (see below).</td>
            </tr>
            <tr>
                <td><code>relu(x)</code></td>
                <td>Function</td>
                <td>Apply relu on each tensor element.</td>
            </tr>
            <tr>
                <td><code>tanh(x)</code></td>
                <td>Function</td>
                <td>Apply a tanh activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>sigmoid(x)</code></td>
                <td>Function</td>
                <td>Apply a sigmoid activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>dropout(x, rate)</code></td>
                <td>Function</td>
                <td>Apply training dropout on tensor <code>x</code> with constant dropout rate hyperparameter <code>rate</code>.</td>
            </tr>
            <tr>
                <td><code>drop(x, rate)</code></td>
                <td>Function</td>
                <td>Shorthand notation <code>dropout</code>.</td>
            </tr>
            <tr>
                <td><code>lrelu(x, slope)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with constant negative slope hyperparameter <code>slope</code>.</td>
            </tr>
            <tr>
                <td><code>prelu(x)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with learnable negative slope.</td>
            </tr>
            <tr>
                <td><code>softmax(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a softmax reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>sum(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a sum reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>mean(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a mean reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>L1(x, dim)</code></td>
                <td>Function</td>
                <td>Apply an L1 normalization on <code>x</code> across dimension <code>dim</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>L2(x, dim)</code></td>
                <td>Function</td>
                <td>Apply an L2 normalization on <code>x</code> across dimension <code>dim</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>max(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a max reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>min(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a min reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols, reg)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
            <tr>
                <td><code>mat(rows, cols)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>matrix</code>.</td>
            </tr>
            <tr>
                <td><code>mat(rows, cols, reg)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>matrix</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter <code>len</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len, reg)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter <code>len</code>, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
            <tr>
                <td><code>vec(len)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>vector</code>.</td>
            </tr>
            <tr>
                <td><code>vec(len, reg)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>vector</code>.</td>
            </tr>
        </tbody>
    </table>

	<h3 id="fastbuilder">3.2. FastBuilder</h3>
	<p>The <code class="language-java">FastBuilder</code> class for building GNN architectures extends the generic
	<code class="language-java">ModelBuilder</code> with common graph neural network operations. The main difference 
	is that it has two constuctor arguments, namely a square matrix <code class="language-java">A</code> that
	is typically a normalization of the (sparse) adjacency matrix, 
	and a feature matrix <code>h0</code>. 
	This builder further supports the notation <code>symbol{l}</code>,
	where the layer counter replaces the symbol part <code>{l}</code> with 0 for the first layer,
	1 for the second, and so on. Prefer the notation <code>h{l}</code> to refer to the node representation
	matrix of the current layer; for the first layer, this is parsed as <code>h0</code>, which is the constant
	set by the constructor.
	<code class="language-java">FastBuilder</code> instances also offer a <code class="language-java">FastBuilder.layer(String)</code>
	chain method to compute neural layer outputs. This is a a variation of operation parsing, where the
	the symbol part <code>{l+1}</code> is substituted with the next layer's counter, 
	the expression is parsed, and the layer counter is incremented by one. Example usage is shown below, where 
	symbolic expressions read similarly to what you would find in a paper.
	</p>

	<pre><code class="language-java">FastBuilder modelBuilder = new FastBuilder(adjacency, features)  // sets A, h0
	.layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden, reg))+vector(hidden))")  // parses h1 = relu(A@(h0	@ ...
	.layer("h{l+1}=A@(h{l}@matrix(hidden, classes, reg))+vector(classes)"); // parses h2 = A@(h1@ ...
	</code></pre>

	<p>Before continuing, let us give some context for the above implementation.
	The base operation of message passing GNNs, which are often used for node classification,
	is to propagate node representations to neighbors via graph edges. Then, neighbors aggregate
	the received representation, where aggregation typically consists of a weighted average per 
	the normalized adjacency matrix's edge weights. For symmetric normalization, this the
	weighted sum is compatible with spectral graph signal processing. The operation to perform
	one propagation can be written as <code class="language-java">.layer("h{l+1}=A @ h{l}")</code>.
	The propagation's outcome is typically transformed further by passing through a dense
	layer.</p>

	<p> Several have been proposed as improvements of this scheme.
	However, they tend to incur marginal accuracy improvements at the cost of more compute. 
	Stay away from complex architectures when learning from large graphs, as JGNN is designed to 
	be lightweight but does not (and is not planned to) leverage GPUs. 
	The library still supports the improvements listed below, since they could be used when
	running time is not a pressing issue (e.g., for transfer or stream learning that applies updates
	for a few epochs), or to analyse smaller graphs: </p>
	
	<ul>
		<li><b>Edge dropout</b> - Applying dropout on the adjacency
		matrix on each layer with <code class="language-java">.layer("h{l+1}=dropout(A,0.5) @ h{l}")</code>. Usage of this operation seems innocuous,
		but it disables a bunch of caching optimizations that occur under-the-hood.</li>
		<li><b>Heterogeneity</b> - Some rcent approaches explicitly account for high-pass frequency diffusion by accounting for the graph Laplacian too. Insert this into the
		architecture as a normal constant like so: <code class="language-java">.constant("L", adjacency.negative().cast(Matrix.class).setMainDiagonal(1))</code></li>
		<li><b>Edge attention</b> - Performs the dot product of edge nodes to create new edge weights
		per the mathematical formula <code>A.(h<sup>T</sup>h)</code>, where
			<code>A</code> is a sparse adjacency matrix, the dot
			<code>.</code> represents the Hadamard product
			(element-by-element multiplication), and
			<code>h</code> is a dense matrix whose rows hold
			respective node representations. JGNN efficiently implements this operation with
			the Neuralang function <code class="language-java">att(A, h)</code>. For
			example, weighted adjacency matrices for each layer of gated attention networks
			are implemented as <code class="language-java">.operation("A{l} = L1(nexp(att(A, h{l})))")</code>.
		</li>
		<li><b>General message passing</b> - JGNN also supports the the fully generized
	message passing scheme between node neighbors of more complex relational analysis
	<a href="https://arxiv.org/pdf/2202.11097.pdf">[Velickovic, 2022]</a>.
	In this generalization, each edge is responsible for appropriately
	transforming and propagating representations to node neighbors;
	create message matrices whose rows correspond to edges and
	columns to edge features by gathering the features of the edge 
	source and destination nodes. Programmatically,obtain edge source indexes
	<code class="language-java">src=from(A)</code> and destination indexes
	<code class="language-java">dst=to(A)</code>, where <code class="language-java">A</code> is
	the adjacency matrix. Then use the horizontal concatenation operation 
	<code>|</code> to concatenate node features.
	One may also concatenate edge features. Given a constructed message, 
	define any kind of ad-hoc
	mechanism or neural processing of messages with
	traditional matrix operations (take care to define correct matrix sizes 
	for dense transformations, e.g., twice the number of
	columns as <code>h{l}</code> in the previous
	snippet). For any kind of <code class="language-java">LayeredBuilder</code>, 
	don't forget that <code">message{l}</code> within 
	operations is needed to obtain a message from the representations 
	<code>h{l}</code> that is not accidentally shared with future layers.
	Receiver mechanisms need to perform some kind of
	reduction on messages. JGNN implements summation
	reduction, given that this has the same
	theoretical expressive power as maximum-based
	reduction but is easier to backpropagate
	through. Perform this like below. 
	The sum is weighted per the values of
	the adjacency matrix <code>A</code>. Thus,
	perform adjacency matrix normalization only if
	you want such weighting to occur.
	

		<pre><code class="language-java">modelBuilder
	.operation("src=from(A)")
	.operation("dst=to(A)")
	.operation("message{l}=h{l}[src] | h{l}[dst]") // has two times the number of h{l}'s features
	.operation("transformed_message{l}=...") // fill in the transformation
	.operation("received{l}=reduce(transformed_message{l}, A)");</code></pre>
		</li>
	</ul>
	
	
	<p>So far, we discussed the propagation mechanisms of
	GNNs, which consider the features of all nodes. However,
	in node classification settings, training data labels
	are typically available only for certain nodes, despite 
	all node features being required to make any prediction. 
	We thus need a mechanism to retrieve the predictions of the top
	layer for those nodes, for example before applying a softmax.
	This is achieved in the snippet below, which uses the gather
	operations through brackets. Alternatively, chain the
	<code class="language-java">FastBuilder.classify()</code> 
	method, which injects this exact code.

	<pre><code class="language-java">modelBuilder
	.var("nodes")
	.layer("h{l} = softmax(h{l})")
	.operation("output = h{l}[nodes]")
	.out("output");</code></pre>

	<p>So far we tackled only 
	<b>equivariant</b> GNNs, whose outputs
	follow any node permutations applied on their inputs.
	In simple terms, if the order of node idenfitifiers is modified
	(both in the graph adjacency matrix and in node feature matrices),
	the order of rows will be similarly modified for outputs.
	Most operations described so far are equivariant (those that are not
	explicitly say so), so that their
	synthesis is also equivariant. However, there 
	are cases where created GNNs should be <b>invariant</b>,
	which means that they should create predictions that remain
	the same despite any input permutations. Invariance is 
	the property to impose when classifying graphs, where
	one prediction should be made for the whole graph.</p>

	<p>Imposing invariance is simple enough; take an equivariant
	architecture and then apply an invariant operation on top.
	You may want to perform further transformations (e.g., some
	dense layers) afterwards, but the general idea remains
	the same. JGNN offers two types of invariant operations, also
	known as pooling:
	reductions and sort-based pooling. Of these, reductions 
	are straightforward to implement
	by taking a dimensionality reduction mechanism (<code>min</code>,
	<code>max</code>, <code>sum</code>, <code>mean</code>) 
	applying it <i>column-wise</i> on the output feature matrix.
	Recall that each row has the features of a different node, 
	so the result of reduction yields an one-dimensional vector that,
	for each feature dimension, aggregates feature values across all nodes.
	</p>

	<p>Reduction-based pooling performs a symmetric operation and
	therefore fail to distinguish between 
	the structural positioning of nodes to be pooled. 
	One computationally light alternative, 
	which JGNN implements, is sorting nodes based on learned features
	before concatenating their features into one vector for
	each graph. This process is further simplified by
	keeping the top <em>reduced</em> number of nodes to
	concatenate their features, where the order is
	determined by an arbitrarily selected feature (in our
	implementation: the last one, with the previous feature
	being used to break ties, and so on).
	The idea is that the selected feature determines
	<em>important</em> nodes whose information can be
	adopted by others. To implement this scheme, JGNN
	provides independent operations to sort nodes, gather
	node latent representations, and reshape matrices into
	row or column tensors with learnable transformations to
	class outputs. These components are demonstrated in the
	following code snippet:
	</p>

		<pre><code class="language-java">long reduced = 5;  // input graphs need to have at least that many nodes
long hidden = 8;  // many latent dims reduce speed without GPU parallelization

ModelBuilder builder = new LayeredBuilder()        
.var("A")  
.config("features", 1)
.config("classes", 2)
.config("reduced", reduced)
.config("hidden", hidden)
.layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden))+vector(hidden))")
.layer("h{l+1}=relu(A@(h{l}@matrix(hidden, hidden))+vector(hidden))")
.concat(2) // concatenates the outputs of the last 2 layers
.config("hiddenReduced", hidden*2*reduced)  // 2* due to concatenation
.operation("z{l}=sort(h{l}, reduced)")  // z{l} are node indexes
.layer("h{l+1}=reshape(h{l}[z{l}], 1, hiddenReduced)")
.layer("h{l+1}=h{l}@matrix(hiddenReduced, classes)")
.layer("h{l+1}=softmax(h{l}, dim: 'row')")
.out("h{l}");</code></pre>
	

	<h3 id="neuralang">3.3. Neuralang</h3>
		
	<p>Neuralang scripts consist of functions that declare machine learning
	components. Use a Rust highlighter to cover all keywords.
	Functions correspond to machine learning modules and call each other.
	At their end lies a <code>return</code> statement, which expresses their
	outcome. All arguments are passed by value, i.e., any assignments are
	performed on fresh variable instances.
	Before explaining how to use the <code class="language-java">Neuralang</code> model builder,
	we present and analyse code that supports a fully functional architecture.
	First, look at the <code>classify</code> 
	function, which for completeness is presented below.
	This takes two tensor inputs: <code>nodes</code> that correspond to identifiers 
	insicating which nodes should be classified (the output has a number of rows equal to the 
	number of identifiers), and a node feature matrix <code>h</code>. 
	It then computes and returns a softmax for the features of the specified nodes. 
	Aside from main inputs, the function's 
	signature also has several configuration values, whose defaults 
	are indicated by a colon <code>:</code> (only configurations have defaults and conversely).
	The same notation is used to 
	set/overwrite configurations when calling functions, as we do for softmax 
	to apply it row-wise. Think of configurations as keyword 
	arguments of typical programming languages, with the difference that
	they control hyperparameters, like dimension sizes or regularization.
	Write exact values for configurations, as for now there no 
	arithmetics take place for them. For example, a configuration 
	<code>patience:2*50</code> creates an error.</p>
		
	<pre><code class="language-rust">fn classify(nodes, h, epochs: !3000, patience: !100, lr: !0.01) {
	return softmax(h[nodes], dim: "row");
}</code></pre>

	<p>Exclamation marks <code>!</code> before numbers broadcast values
	to all subsequent function calls that have configurations with the same 
	name. The broadcasted defaults overwrite already existing defaults of configurations with the same
	name anywhere in the code. All defaults are replaced by values explicitly set when calling functions.
	For example, take advantage of this prioritization to force output layer dimensions match your data. Importantly, 
	broadcasted values are stored within JGNN's <code class="language-java">Neuralang</code> model 
	builder too; this is useful for Java integration, for example to retrieve learning training hyperparameters
	from the model. To sum up, configuration values have the following priority, from strongest to weakest:<br> 
	1. Arguments set during the function's call.<br>
	2. Broacasted configurations (the last broadcasted value, including configurations set by Java).<br>
	3. Function signature defaults.<br>
	</p>

	<p>Next, let us look at some functions creating the main body of an architecture.
	First, <code>gcnlayer</code> accepts 
	two parameters: an adjacency matrix <code>A</code> and input feature matrix <code>h</code>.
	The configuration <code>hidden: 64</code> in the functions's signature 
	specifies the deafult number of hidden units,
	whereas <code>reg: 0.005</code> is the L2 regularization applied 
	during machine learning. The questionmark <code>?</code> 
	in matrix definitions lets the autosize feature of JGNN determine 
	dimension sizes based on a test run - if possible. 
	Finally, the function returns the activated output of a
	GCN layer. Similarly, look at the <code>gcn</code> function. This declares
	the GCN architecture and has as configuration the number of output classes. 
	The function basically consists of two <code>gcnlayer</code> layers,
	where the second's hidden units are set to the value of output classes. 
	The number of classes is unknown as of writting the model, and thus is externally declared 
	with the <code>extern</code> keyword to signify that this value should always by provided 
	by Java's side of the implementation.</p>

	<pre><code class="language-rust">fn gcnlayer(A, h, hidden: 64, reg: 0.005) {
	return A@h@matrix(?, hidden, reg) + vector(hidden);
}
fn gcn(A, h, classes: extern) {
	h = gcnlayer(A, h);
	h = dropout(relu(h), 0.5);
	return gcnlayer(A, h, hidden: classes);
}
</code></pre>


	<p>We now move to parsing our declarations with the <code class="language-java">Neuralang</code>
	model builder and using them to create an architecture. To this end, save your code
	to a file and get is as a path <code class="language-java">Path architecture = Paths.get("filename.nn");</code>,
	or avoid external files by inlining the definition within Java code through 
	a multiline string per <code class="language-java">String architecture = """ ... """;</code>.
	Below, this string is parsed within a functional programming chain, where
	each method call returns the modelBuilder instance to continue calling more methods. 
	</p>


	<p>For the model builder, the following snippet sets remaining hyperparameters 
	and overwrites the default value
	for <code class="language-java">"hidden"</code>. It also specifies
	that certain variables are constants, namely the adjacency matrix <code>A</code> and node 
	representation <code>h</code>, as well as that node identifiers is a variable that serves 
	as the architecture's inpu. There could be multiple inputs, so this distinction of what 
	is a constant and what is a variable depends mostly on which quantities change 
	during training and is managed by onlt the Java-side of the code. 
	In the case of node classification, both the adjacency matrix and
	node features remain constant, as we work in one graph. Finally, the definition
	sets an Neuralang expression as the architecture's output
	by calling the <code class="language-java">.out(String)</code> method,
	and applies the <code class="language-java">.autosize(Tensor...)</code> method to infer hyperparameter
	values denoted with <code class="language-java">?</code> from an example input.
	For faster completion of the model, provide a dataless list of node identifiers as input,
	like below.</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new Neuralang()
	.parse(architecture)
	.constant("A", dataset.graph())
	.constant("h", dataset.features())
	.var("nodes")
	.config("classes", numClasses)
	.config("hidden", numClasses+2)  // custom number of hidden dimensions
	.out("classify(nodes, gcn(A,h))")  // expression to parse into a value
	.autosize(new EmptyTensor(numSamples));

System.out.println("Preferred learning rate: "+modelBuilder.getConfig("lr"));</code></pre>
	
	
	<h3 id="debugging">3.4. Debugging</h3>
	<p>JGNN offers high-level tools for debugging
		architectures. Here we cover what diagnostics to run, and how to make
		sense of error messages to fix erroneous
		architectures. We already mentioned that model builder
		symbols should be assigned to before
		subsequent use. For example, consider a <code class="language-java">FastBuilder</code> that
		tries to parse the expression <code class="language-java">.layer("h{l+1}=relu(hl@matrix(features, 32, reg)+vector(32))")</code>, 
		where <code>hl</code> is a typographical error of
		<code>h{l}</code>. In this case, an exception is thrown:
		<code>Exception in thread "main" java.lang.RuntimeException: Symbol hl not defined.</code>

		Internally, models are effectively directed acyclic graphs (DAGs)
		that model builders create. DAGs should not be confused with the graphs
		that GNNs architectures analyse; they are just an organization of data flow
		between <code>NNComponent</code>s. During parsing, builders
		may create temporary variables, which start with
		the <code>_tmp</code> prefix and are followed by
		a number. Temporary variables often link
		components to others that use them.
		The easiest way to understand execution DAGs is
		to look at them. The library provides two tools
		for this purpose: a <code class="language-java">.print()</code> method
		that prints built functional flows in the system
		console, and a <code class="language-java">.getExecutionGraphDot()</code> 
		method that returns a string holding the execution graph in
		<em>.dot</em> format for visualization with
		tools like <a href="https://dreampuf.github.io/GraphvizOnline" target="_blank">GraphViz</a>.
	</p>

	<p>Another error-checking procedure consists of
		an assertion that all model operations eventually affect
		at least one output. Computational branches that lead nowhere mess up the
		DAG traversal during backpropagation and should be checked with the
		method <code class="language-java">.assertBackwardValidity()</code>.
		The latter throws an exception if an invalid model is found.
		Performing this assertion early on in
		model building will likely throw exceptions that
		are not logical errors, given that independend
		outputs may be combined later. Backward validity errors 
		look like this the following example. This 
		indicates that the component
		<code class="language-java">_tmp102</code> does not lead to an output,
		and we should look at the execution tree to
		understand its role.</p>

	<pre><code>Exception in thread "main" java.lang.RuntimeException: The component class mklab.JGNN.nn.operations.Multiply: _tmp102 = null does not lead to an output
at mklab.JGNN.nn.ModelBuilder.assertBackwardValidity(ModelBuilder.java:504)
at nodeClassification.APPNP.main(APPNP.java:45)</code></pre>

	
			<p>Some tensor or matrix methods do not 
			correspond to numerical operations but
			are only responsible for naming dimensions.
			Functionally, such methods are largely decorative,
			but they cab improve debugging by throwing errors for
			incompatible non-null names. For example,
			adding two matrices with different dimension
			names will result in an error. Likewise, the
			inner dimension names during matrix
			multiplication should agree. 
			Arithmetic operations, <em>including</em>
			matrix multiplication and copying,
			automatically infer dimension names in the
			result to ensure that only compatible data types
			are compared. Dimension name changes 
			do <em>not</em>
			backtrack the changes, even for see-through
			data types, such as the outcome of
			<code class="language-java">asTransposed()</code>.
			Matrices effectively have three
			dimension names: for their rows, columns,
			and inner data as long as they are treated
			as tensors.</p>

			<table class="table table-bordered">
				<thead>
					<tr>
						<th>Operation</th>
						<th>Comments</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><code class="language-java">Tensor setDimensionName(String name)</code></td>
						<td>For naming tensor dimensions (of
							the 1D space tensors lie
							in).</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setRowName(String rowName)</code></td>
						<td>For naming what kind of
							information matrix rows hold
							(e.g., "samples"). Defined only to matrices.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setColName(String colName)</code></td>
						<td>For naming what kind of
							information matrix columns hold
							(e.g., "features"). Defined only for matrices.</td>
					</tr>
					<tr>
						<td><code class="language-java">Tensor setDimensionName(String rowName, String colName)</code></td>
						<td>A shorthand of calling
							<code class="language-java">setRowName(rowName).setColName(colName)</code>. Defined only for matrices.
						</td>
					</tr>
				</tbody>
			</table>

	<p>There are two main mechanisms for identifying
	logical errors within architectures: a) mismatched
	dimension size, and b) mismatched dimension
	names. Of the two, dimension sizes are easier to
	comprehend since they just mean that operations
	are mathematically invalid.
	On the other hand, dimension names need to be
	determined for starting data, such as model
	inputs and parameters, and are automatically
	inferred from operations on such primitives. For
	in-line declaration of parameters in operations
	or layers, dimension names are copied from any
	hyperparameters. Therefore, for easier
	debugging, prefer using functional expressions
	that declare hyperparameters, like below.</p>

	<pre><code class="language-java">new ModelBuilder()
	.config("features", 7)
	.config("hidden", 64)
	.var("x")
	.operation("h = x@matrix(features, hidden)");</code></pre>

	<p>Both mismatched dimensions and mismatched
		dimension names throw runtime exceptions. The
		beginning of their error console traces should
		start with something like this:</p>

	<pre><code>java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null with the following inputs:
class mklab.JGNN.nn.activations.Relu: h1 = SparseMatrix (3327,32) 52523/106464 entries
class mklab.JGNN.nn.inputs.Parameter: _tmp5 = DenseMatrix (64, classes 6)
java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
at mklab.JGNN.core.Matrix.matmul(Matrix.java:258)
at mklab.JGNN.nn.operations.MatMul.forward(MatMul.java:21)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:180)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
	...</code></pre>

	<p>This particular stack trace tells us that the architecture
	encounters mismatched matrix sizes when trying
	to multiply a 3327x32 SparseMatrix with a 64x6
	dense matrix. Understanding the exact error is
	easy—the inner dimensions of matrix
	multiplication do not agree. However, we need to
	find the error within our architecture to fix
	it. To do this, the error message message states 
	that the error occures.
	<code>During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null</code>. This tells us that the problem
	occurs when trying to calculate
	<code>_tmp4</code>, which is currently assigned
	a <code class="language-java">null</code> tensor as value. Some more
	information is available to see what the
	operation's inputs are like—in this case, they
	coincide with the multiplication's inputs, but
	this will not always be the case.
	The important point is to go back to the
	execution tree and see during which exact
	operation this variable is defined. There, we
	will undoubtedly find that some dimension had 64
	instead of 32 elements or conversely.</p>

	<p>In addition to all other debugging mechanisms,
	JGNN presents a way to show when forward and
	backward operations of specific code components
	are executed and with what kinds of arguments.
	This can be particularly useful when testing new
	components in real (complex) architectures.
	The practice consists of calling a
	<code class="language-java">monitor(...)</code> function within
	operations. This does not affect what
	expressions do and only enables printing
	execution tree operations on operation
	components. For example, the next snippet
	monitors the outcome of matrix multiplication:
	</p>
	<pre><code class="language-java">builder.operation("h = relu(monitor(x@matrix(features, 64)) + vector(64))")</code></pre>
		
	</section>

	<section id="training">
		<h1>4. Training</h1>
		<p>Here we describe how to train a JGNN model created
		with the previous section's builders.
		Broadly, we need to load some reference data and employ an optimization scheme
		to adjust trainable parameter values based on the differences between desired
		and current outputs. To this end, we 
		start by describing generic patterns for creating graph and node
		feature data, and then move to specific data organizations
		for the tasks of node classification and graph classification.
		These tasks have helper classes that implement common training schemas
		(reach out with requests for helper classes for other kinds of predictive tasks 
		in the project's GitHub issues).</p>

		<h3 id="create-data">4.1. Create data</h3>
		<p>JGNN contains dataset classes that automatically download and load 
		datasets for out-of-the-box experimentation. These datasets can be found
		in the <a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/adhoc/datasets/package-summary.html" target="_blank">
		adhoc.datasets</a> Javadoc, and we already covered their usage patterns. 
		In practice, though, you will want to 
		use your own data. In the simplest case, both the number of nodes or data samples, and
		the number of feature dimensions are known beforehand. If so, create
		dense feature matrices with the following code. This uses the 
		minimum memory necessary to construct the feature matrix. If
		features are dense (do not have a lot of zeros),
		consider using the <code class="language-java">DenseMatrix</code> class
		instead of initializing a sparse matrix, like below.</p>

			<pre><code class="language-java">Matrix features = new SparseMatrix(numNodes, numFeatures);
for(long nodeId=0; nodeId&lt;numNodes; nodeId++)
	for(long featureId=0; featureId&lt;numFeatures; featureId++)
		features.put(nodeId, featureId, 1);</code></pre>
		</section>

		<p>Sometimes, it is easier to read node or sample features
		line-by-line, for instance, when reading a <em>.csv</em>
		file. In this case, store each line as a separate tensor.
		Convert a list of tensors representing row vectors into a
		feature matrix like in the example below.</p>

			<pre><code class="language-java">ArrayList<Tensor> rows = new ArrayList<Tensor>();
try(BufferedReader reader = new BufferedReader(new FileReader(file))){
	String line = reader.readLine();
	while (line != null) {
		String[] cols = line.split(",");
		Tensor features = new SparseTensor(cols.length);
		for(int col=0;col&lt;cols.length;col++)
			features.put(col, Double.parseDouble(cols[col]));
		rows.add(features);
		line = reader.readLine();
	}
}
Matrix features = new WrapRows(rows).toSparse();</code></pre>

			
		<p>Creating adjacency matrices is similar to creating
		preallocated feature matrices. When in doubt, use the <b>sparse</b>
		format for adjacency matrices, as the allocated memory of dense
		counterparts scales qudratically to the number of nodes. Note that many GNNs
		consider bidirectional (i.e., non-directed) edges, in which case	
		both directions should be added to the adjacency. Use the following snippet as a
		template. Recall that JGNN follows a function chain notation, so each modification
		returns the <code class="language-java">matrix</code> instance.
		Don't forget to normalize or apply the renormalization trick (self-edges) on matrices if these
		are needed by your architecture, for instance by calling
		<code class="language-java">adjacency.setMainDiagonal(1).setToSymmetricNormalization();</code>
		after matrix creation.</p>

		<pre><code class="language-java">Matrix adjacency = new SparseMatrix(numNodes, numNodes);
for(Entry&lt;Long, Long&gt; edge : edges)
	matrix
		.put(edge.getKey(), edge.getValue(), 1)
		.put(edge.getValue(), edge.getKey(), 1);</code></pre>
		
	
		<p>All tensor operations can be viewed in the 
		<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/core/Tensor.html" target="_blank">core.tensor</a>
		and <a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/core/Matrix.html" target="_blank">core.matrix</a>
		Javadoc. The <code class="language-java">Matrix</code> class extends the concept
		of tensors with additional operations, like transposition, matrix multiplication,
		and row and column access. Under the
		hood, matrices linearly store elements and use
		computations to transform the (row, col)
		position of their elements to respective
		positions. The outcome of some methods inherited
		from tensors may need to be typecast back into a
		matrix (e.g., for all in-place operations).
		
		Operations can be split into arithmetics that combine the values
		of two tensors to create a new one (e.g., <code class="language-java">Tensor add(Tensor)</code>),
		in-place arithmetics that alter a tensor without creating
		a new one (e.g., <code class="language-java">Tensor selfAdd(Tensor)</code>),
		summary statistics that output simple numeric values (e.g., <code class="language-java">double Tensor.sum()</code>),
		and element getters and setters.
		In-place arithmetics follow the same naming
		conventions of base arithmetics but their method names begin with a "self"
		prefix for pairwise operations and a "setTo" prefix
		for unary operations. Since they do not allocate new memory,
		prefer them for intermediate calculation steps.
		For example, the following code can be
		used for creating and normalizing a tensor of
		ones without using any additional memory.</p>

		<pre><code class="language-java">Tensor normalized = new DenseTensor(10)
	.setToOnes()
	.setToNormalized();</code></pre>

		<p>Initialize a dense or sparse tensor -both of which represent one-dimensional vectors- with its number
		of elements. If there are many zeros expected,
		prefer using a sparse tensor. For example, one-hot encodings for classification
			problems can be generated with the following
			code. This creates a dense tensor with
			<code class="language-java">numClasses</code> elements and puts at
			element <code class="language-java">classId</code> the value 1:
		</p>

		<pre><code class="language-java">int classId = 1;
int numClasses = 5;
Tensor oneHotEncoding = new mklab.JGNN.tensor.DenseTensor(numClasses).set(classId, 1); // creates the tensor [0,1,0,0,0]</code></pre>
			

		<p>The above snippets all make use of numerical node identifiers. To
		manage these, JGNN provides an <code class="language-java">IdConverter</code> class; 
		convert hashable objects (typically strings) to identifiers by calling
		<code  class="language-java">IdConverter.getOrCreateId(object)</code>. Also use
		converters to one-hot encode class labels. To search only for previously
		registered identifiers, use <code class="language-java">IdConverter.get(object)</code>.
		For example, construct a label matrix with the following snippet.
		In this, <code class="language-java">nodeLabels</code> is a dictionary
		from node identifiers to node labels that is being converted to a sparse matrix.</p>

		<pre><code class="language-java">IdConverter nodeIds = new IdConverter();
IdConverter classIds = new IdConverter();
for(Entry&lt;String, String&gt; entry : nodeLabels) {
	nodeids.getOrCreateId(entry.getKey());
	classIds.getOrCreateId(entry.getValue());
}
Matrix labels = new SparseMatrix(nodeIds.size(), classIds.size());
for(Entry&lt;String, String&gt; entry : nodeLabels) 
	labels.put(nodeids.get(entry.getKey()), classIds.get(entry.getValue()), 1);</code></pre>

		<p>Reverse-search the converter to obtain the original object
		of predictions per <code class="language-java">IdConverter.get(String)</code>. The following example
		accesses one row of a label matrix, performs and argmax operation to find the position of the 
		maximum element, and reconstruct the label for the corresponding row with reverse-search.
		</p>

		<pre><code class="language-java">long nodeId = nodeIds.get("nodeName");
Tensor prediction = labels.accessRow(nodeId);
long predictedClassId = prediction.argmax();
System.out.println(classIds.get(predictedClassId));</code></pre>
			

		<h3 id="node-classification">4.2. Node classification</h3>
		<p>
		Node classification models can be backpropagated by considering a list of node indeces and desired
		predictions for those nodes. We first show an automation of the training process that controls
		it in a predictable manner.
		</p>

		<b>This section is under construction.</b>
		
		<pre><code class="language-java">Slice nodes = dataset.samples().getSlice().shuffle(100);  // or nodes = new Slice(0, numNodes).shuffle(100);
Model model = modelBuilder()
	.getModel()
	.init(new XavierNormal())
	.train(trainer,
			nodes.samplesAsFeatures(), 
			dataset.labels(), 
			nodes.range(0, trainSplit), 
			nodes.range(trainSplit, validationSplit));
		</code></pre>



		<h3 id="graph-classification">4.3. Graph classification</h3>


		<p>Most neural network architectures are designed with the idea
		of learning to classify nodes or samples. However, GNNs also
		provide the capability to classify entire graphs based on
		their structure. To define architectures for graph classification, we use
		the generic <code class="language-java">LayeredBuilder</code> class. The main
		difference compared to traditional neural networks is
		that architecture inputs do not all exhibit the same
		size (e.g., some graphs may have more nodes than others)
		and therefore cannot be organized into tensors of common
		dimensions. Instead, assume that training data are stored in the
		following lists:</p>

			<pre><code class="language-java">ArrayList<Matrix> adjacencyMatrices = new ArrayList<Matrix>();
ArrayList<Matrix> nodeFeatures = new ArrayList<Matrix>();
ArrayList<Tensor> graphLabels = new ArrayList<Tensor>();</code></pre>

		<p>The <code class="language-java">LayeredBuilder</code> class introduces the
		input variable <code class="language-java">h<sub>0</sub></code> for sample
		features. We can use it to pass node features to the
		architecture, so we only need to add a second input
		storing the (sparse) adjacency matrix:</p>

		<pre><code class="language-java">.var("A")</code></pre>

		<p>We can then proceed to define a GNN architecture, for
		instance as explained in previous tutorials. This time,
		though, we aim to classify entire graphs rather than
		individual nodes. For this reason, we need to pool top
		layer node representations, for instance by averaging
		them across all nodes:</p>

		<pre><code class="language-java">.layer("h{l+1}=softmax(mean(h{l}, dim: 'row'))")</code></pre>

		<p>Finally, we need to set up the top layer as the built
		model's output per: <code class="language-java">.out("h{l}")</code>
		An example architecture following these principles follows:</p>

                <pre><code class="language-java">ModelBuilder builder = new LayeredBuilder()
    .var("A")  
    .config("features", nodeLabelIds.size())
    .config("classes", graphLabelIds.size())
    .config("hidden", 16)
    .layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden)))") 
    .layer("h{l+1}=relu(A@(h{l}@matrix(hidden, classes)))") 
    .layer("h{l+1}=softmax(mean(h{l}, dim: 'row'))")
    .out("h{l}");</code></pre>

			<p>For the time being, training architectures like the above
				on prepared data requires manually calling the
				backpropagation for each epoch and each graph in the
				training batch. To do this, first retrieve the model and
				initialize its parameters:</p>

			<pre><code class="language-java">Model model = builder.getModel()
	.init(new XavierNormal());</code></pre>

			<p>Next, define a loss function and set up a batch
				optimization strategy wrapping any base optimizer and
				accumulating parameter updates until
				<code class="language-java">BatchOptimizer.updateAll()</code> is called later
				on:
			</p>

			<pre><code class="language-java">Loss loss = new CategoricalCrossEntropy();
BatchOptimizer optimizer = new BatchOptimizer(new Adam(0.01));</code></pre>

			<p>Finally, training can be conducted by iterating through
				epochs and training samples and appropriately calling
				the <code class="language-java">Model.train</code> for combinations of node
				features and graph adjacency matrix inputs and graph
				label outputs. At the end of each batch (e.g., each
				epoch), don't forget to call the
				<code class="language-java">optimizer.updateAll()</code> method to apply the
				accumulated gradients. This process can be realized with
				the following code:
			</p>

			<pre><code class="language-java">for(int epoch=0; epoch&lt;300; epoch++) {
	for(int graphId=0; graphId&lt;graphLabels.size(); graphId++) {
		 Matrix adjacency = adjacencyMatrices.get(graphId);
		 Matrix features = nodeFeatures.get(graphId);
		 Tensor label = graphLabels.get(graphId);
		 model.train(loss, optimizer, 
			  Arrays.asList(features, adjacency), 
			  Arrays.asList(label));
	}
	optimizer.updateAll();
}</code></pre>

		<p>To speed up graph classification, use JGNN's
		parallelization capabilities to calculate gradients
		across multiple threads. Parallelization for node
		classification holds little meaning, as the same
		propagation mechanism needs to be run on the same graph
		in parallel. However, this process yields substantial
		speedup for the <em>graph</em> classification
		problem. Parallelization can use JGNN's thread pooling to
		perform gradients, wait for the conclusion of submitted
		tasks, and then apply the accumulated gradient updates. 
		This is achieved through a batch optimizer that accumulates
		gradients in the following example:</p>

                <pre><code class="language-java">for(int epoch=0; epoch&lt;500; epoch++) {
    // gradient update
    for(int graphId=0; graphId&lt;dtrain.adjucency.size(); graphId++) {
        int graphIdentifier = graphId;
        ThreadPool.getInstance().submit(new Runnable() {
            @Override
            public void run() {
                Matrix adjacency = dtrain.adjucency.get(graphIdentifier);
                Matrix features= dtrain.features.get(graphIdentifier);
                Tensor graphLabel = dtrain.labels.get(graphIdentifier).asRow();  
                model.train(loss, optimizer, 
		            Arrays.asList(features, adjacency), 
		            Arrays.asList(graphLabel));
            }
        });
    }
    ThreadPool.getInstance().waitForConclusion();  // waits for all gradients to finish calculating
    optimizer.updateAll();

    double acc = 0.0;
    for(int graphId=0; graphId&lt;dtest.adjucency.size(); graphId++) {
        Matrix adjacency = dtest.adjucency.get(graphId);
        Matrix features= dtest.features.get(graphId);
        Tensor graphLabel = dtest.labels.get(graphId);
        if(model.predict(Arrays.asList(features, adjacency)).get(0).argmax()==graphLabel.argmax())
           acc += 1;
        System.out.println("iter = " + epoch + "  " + acc/dtest.adjucency.size());
    }
}</code></pre>
    </section>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-rust.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>