<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JGNN</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-vs.min.css"
        integrity="sha512-Jn4HzkCnzA7Bc+lbSQHAMeen0EhSTy71o9yJbXZtQx9VvozKVBV/2zfR3VyuDFIxGvHgbOMMNvb80l+jxFBC1Q=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        body {
            display: flex;
            overflow-x: hidden; /* Prevent horizontal scrolling */
        }
		
		.table td, .table th {
		  min-width: 100px; /* Adjust the value as needed */
		}

        #sidebar {
            position: fixed;
            top: 0;
            left: 0;
            height: 100%;
            width: 190px;
            background-color: #f8f9fa;
            padding-top: 20px;
            padding-left: 5px;
            border-right: 1px solid #ddd;
        }

        .sidebar-title {
            text-align: center;
        }

        #content {
            margin-left: 200px;
            padding: 20px;
            flex-grow: 1;
            overflow-wrap: break-word;
            word-wrap: break-word;
            word-break: break-word;
            width: 100%; /* Ensures content takes full width */
        }

        section {
            margin-top: 90px;
        }

        .sidebar-link {
            color: #000;
            text-decoration: none;
            padding: 10px;
            display: block;
        }

        .sidebar-link.active {
            color: #fff;
            background-color: #33bbff;
            border-radius: 4px;
        }
        .sidebar-link.subsection {
            color: #777;
        }
        .sidebar-link.subsection.active {
            color: #fff;
        }


        details {
            border: 1px solid #ccc;
            border-radius: 5px;
            padding: 10px;
            background-color: #f8f9fa;
            transition: all 0.3s ease-in-out;
            margin-bottom: 1em;
        }

        details[open] {
            background-color: #e9ecef;
        }

        summary {
            cursor: pointer;
            list-style: none;
        }

        summary::before {
            content: "▶ ";
            font-size: 14px;
            color: #007bff;
            transition: transform 0.2s;
            display: inline-block;
            margin-right: 10px;
        }

        details[open] summary::before {
            content: "▼ ";
            margin-bottom: 1em;
        }

        @media (max-width: 767.98px) {
            #sidebar {
                display: none;
            }

            .navbar {
                position: fixed;
                display: block;
                color: #fff;
                background-color: #007bff;
                border-radius: 4px;
            }

            #content {
                margin-left: 0;
                padding: 20px;
                flex-grow: 1;
                overflow-wrap: break-word;
                word-wrap: break-word;
                word-break: break-word;
            }
        }

        @media (min-width: 768px) {
            .navbar {
                display: none;
            }
            #content {
                max-width: 800px;
            }
        }
    </style>
    <script>
		document.addEventListener("DOMContentLoaded", function () {
			const sidebarLinks = document.querySelectorAll('.sidebar-link');
			const sections = document.querySelectorAll('section');

			function removeActiveClasses() {
				sidebarLinks.forEach(link => link.classList.remove('active'));
			}

			function addActiveClass(link) {
				removeActiveClasses();
				if (link) link.classList.add('active');
			}

			function getClosestSection() {
				let minDistance = Infinity;
				let closestSection = null;
				const scrollY = window.scrollY;
				sections.forEach(section => {
					const offset = Math.abs(section.offsetTop - scrollY - 100); // Adjust for fixed navbar offset
					if (offset < minDistance) {
						minDistance = offset;
						closestSection = section;
					}
				});
				return closestSection;
			}

			function updateActiveSection() {
				const section = getClosestSection();
				if (section) addActiveClass(document.querySelector(`.sidebar-link[href="#${section.id}"]`));
			}
			window.addEventListener("scroll", updateActiveSection);
			sidebarLinks.forEach(link => {link.addEventListener("click", function (event) {
				event.preventDefault();
				const targetId = this.getAttribute("href").substring(1);
				const targetSection = document.getElementById(targetId);

				if (targetSection) {window.scrollTo({
					top: targetSection.offsetTop - 80,
					behavior: "smooth"
				});}
			});});
			window.addEventListener("load", updateActiveSection);
		});
		</script>


</head>

<body>
    <nav id="sidebar">
        <ul class="nav flex-column">
            <li class="nav-item sidebar-title"><h5>JGNN</h5></li>
            <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
            <li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#modelbuilder">3.1. ModelBuilder</a></li>
            <li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#fastbuilder">3.2. FastBuilder</a></li>
			<li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#neuralang">3.3. Neuralang</a></li>
            <li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#debugging">3.4. Debugging</a></li>
            <li class="nav-item"> <a class="sidebar-link" href="#training">4. Training</a></li>
			<li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#create-data">4.1. Create data</a></li>
            <li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#node-classification">4.2. Node classification</a></li>
			<li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#graph-classification">4.3. Graph classification</a></li>
			<li class="nav-item ps-md-3"><a class="sidebar-link small p-1 subsection" href="#losses-and-optimizers">4.4. Losses and optimizers</a></li>
        </ul>
    </nav>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <div class="container-fluid">
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item"> <a class="sidebar-link" href="#setup">1. Setup</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#quickstart">2. Quickstart</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#gnn-builders">3. GNN Builders</a></li>
                    <li class="nav-item"> <a class="sidebar-link" href="#training">4. Training</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <div id="content">
	<h1 class="text-center">JGNN</h1>
	<div class="text-center m-3">
		<a href="https://mklab-iti.github.io/JGNN/javadoc/" target="_blank" class="btn btn-warning">Javadoc</a>
		<a href="https://github.com/MKLab-ITI/JGNN" target="_blank" class="btn btn-warning">GitHub</a>
		<a href="https://github.com/MKLab-ITI/JGNN/tree/main/tutorials" target="_blank" class="btn btn-warning">Tutorials</a>
	</div>
	<p class="text-center"><em>Resource efficient machine learning and graph neural networks in native Java.</em>
	</p>

	<p>Graph Neural Networks (GNNs) are getting more and more popular,
	for example to make predictions
	based on relational information, or to perform inference on small datasets. JGNN is a library that
	provides cross-platform implementations of this paradigm and traditional neural networks
	without the need for dedicated hardware or firmware; create highly portable models that fit and
	are trained in a few megabytes of memory.
	</p>

	<p>
	Keep in mind that this is not a library
	for computationally intensive architectures; it does support multiple CPU cores and contains
	highly optimized code, but has no GPU support and we do not plan to
	add any (unless such support gets integrated in the Java virtual machine). So, while complex architectures
	are supported and scale to graphs with many nodes, running them fastly requires compromises in the number of learned
	parameters or running time.
	</p>

	<section id="setup">
	<h1>1. Setup</h1>
	<p>The simplest way to set up JGNN is to download it as a JAR package from
	<a href="https://github.com/MKLab-ITI/JGNN/releases">releases</a> 
	and add it your Java project's build path. Those working with Maven 
	or Gradle can instead add JGNN's latest nightly release as a dependency from its JitPack 
	distribution. Follow the link below for full instructions.<br>
	<a href="https://jitpack.io/#MKLab-ITI/JGNN"><img src="https://jitpack.io/v/MKLab-ITI/JGNN.svg" alt="download JGNN"></a>
	</p>
	<p>
	For example, the fields in the snippet below may be added in a Maven <em>pom.xml</em> file 
	to work with the latest nightly release.</p>
	<pre><code class="language-xml">&lt;repositories&gt;
	&lt;repository&gt;
		&lt;id&gt;jitpack.io&lt;/id&gt;
		&lt;url&gt;https://jitpack.io&lt;/url&gt;
	&lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
	&lt;dependency&gt;
		&lt;groupId&gt;com.github.MKLab-ITI&lt;/groupId&gt;
		&lt;artifactId&gt;JGNN&lt;/artifactId&gt;
		&lt;version&gt;SNAPSHOT&lt;/version&gt;
	&lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
	</section>
	
	<section id="quickstart">
	<h1>2. Quickstart</h1>

	<p>Here we demonstrate JGNN for node classification. This is an inductive learning
	task that fills in node labels given a graph's structure, node features, and some already known
	labels. Classifying graphs is also supported, although it is a harder task to explain details.
	</p>

	<p>
	GNN architectures are typically written
	as message passing mechanisms; they diffuse node representations across edges, where
	node neighbors pick up, aggregate (e.g., average), and transform
	incoming representations to update theirs. Alternatives that boast higher 
	expressive power also exist and are supported, but simple architectures 
	may be just as good or better than complex alternatives in solving 
	practical problems <a href="https://www.mdpi.com/2076-3417/14/11/4533">[Krasanakis et al., 2024]</a>.
	Simple architectures also enjoy reduced resource consumption.</p>
	
	<p>Start with the following commands that load the <code class="language-java">Cora</code> dataset from those shipped
	with the library for out-of-the-box testing. The first time an instance of this dataset is created,
	it downloads raw data from a web resource and stores them in a local <code class="language-java">downloads/</code>
	folder. The data are then loaded into a sparse graph adjacency matrix, a dense node feature matrix,
	and a dense node label matrix.
	Each row in those matrices contains the corresponding node's
	neighbors, features, or one-hot encoding of labels.</p>

	<p>The second command applies the renormalization trick and
	symmetric normalization on the adjacency matrix; these respectively make GNN computations numerically stable by adding self-loops
	to all nodes and enable graph spectral theory properties. The in-place variation of those operations is used for minimal memory footprint.</p>
	
	<pre><code class="language-java">Dataset dataset = new Cora();
dataset.graph().setMainDiagonal(1).setToSymmetricNormalization();</code></pre>

	<p class="alert alert-info d-flex align-items-center" role="alert">
		<i class="bi bi-info-circle-fill me-2 fs-3"></i>
		Sparse and dense representations are interoperable. Sparsity is more efficient for lots of zeros,
		so internal computations automatically select matrix types for outcomes.
	</p>

	
	<p>We now incrementally create a trainable model using symbolic expressions that resemble math 
	notation. The expressions are part of a scripting language called <i>Neuralang</i>. For fast onboarding, stick to
	the <code class="language-java">FastBuilder</code> class for creating models, which ommits some of
	the language's features in favor of providing programmatic shortcuts for boilerplate. Its constructor
	accepts two arguments <code>A</code> and <code>h0</code>, respectivel holding
	the graph's adjacency matrix and node features. These are set as constant symbols and can be used in expressions;
	other constants and input variables can be set too,
	but more on this later.</p>

	<p>
	JGNN promotes functional programming method chains, where the builder's instance is returned by each of
	its methods. Below we use this pattern to implement the Graph Convolutional Network (GCN)
	architecture <a href="https://arxiv.org/abs/1609.02907">[Kipf and Welling, 2017]</a>.
	For the time being, notice the <code>matrix</code> and <code>vector</code> functions in scripted expressions;
	these inline declarations of learnable parameter for
	given dimensions and regularization.
	</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new FastBuilder(dataset.graph(), dataset.features())
	.config("reg", 0.005)
	.config("classes", numClasses)
	.config("hidden", 64)
	.function("gcnlayer", "(A,h){Adrop = dropout(A, 0.5); return Adrop@(h@matrix(?, hidden, reg))+vector(?);}")
	.layer("h{l+1}=relu(gcnlayer(A, h{l}))")
	.config("hidden", "classes")  // reassigns the output gcnlayer's "hidden" to be equal to the number of "classes"
	.layer("h{l+1}=gcnlayer(A, h{l})")
	.classify()
	.autosize(new EmptyTensor(numSamples));</code></pre>

	<details><summary>Commonly used methods of <code class="language-java">FastBuilder</code>.</summary>

	<ul>
		<li><code>config</code> - Configures hyperparameter values. These can be used in all subsequent function and layer declarations.</li>
		<li><code>function</code> - Declares a Neuralang function, in this case with inputs <code>A</code> and <code>h</code>.</li>
		<li><code>layer</code> - Declares a layer that can use builtin and Neuralang functions. In this, the symbols <code>{l}</code> and <code>{l+1}</code> specifically are replaced by a layer counter.</li>
		<li><code>classify</code> - Adds a softmax layer tailored to classification. This also silently declares an input <code>nodes</code> that represents a list of node indices where the outputs should be computed.</li>
		<li><code>autosize</code> - Automatically sizes matrix and vector dimensions that were originally defnoted with a questionmark <code>?</code>. This method requires some input example, and here we provide a list of node identifiers, which we also make dataless (have only the correct dimensions without allocating memory). This method also checks for integrity errors in the declared architecture, such as computational paths that do not lead to an output.</li>
	</ul>
	</details>
	
	
	<p>Training can be implemented
	manually, by using inputs to compute outputs on the built model, computing losses, and triggering backpropagation
	on an optimizer. JGNN automates common
	patterns by extending a base <code class="language-java">ModelTraining</code> class with strategies
	tailored to different data formats and predictive tasks. Find these subclasses in the 
	<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/adhoc/train/package-summary.html" target="_blank">adhoc.train</a> 
	Javadoc. Instances of model trainers
	accept chain notation to set parameters like training and validation data
	(these should be created first and depend on the model training class) and aspects of the learning strategy like the number of epochs, patience
	for early stopping, the employed optimizer, and loss functions. An example is presented below.</p>

	<pre><code class="language-java">Slice nodes = dataset.samples().getSlice().shuffle(); // permutes node int ids
Matrix inputFeatures = Tensor.fromRange(nodes.size()).asColumn();
Loss loggingLoss = new VerboseLoss(new CategoricalCrossEntropy(), new Accuracy())
	.setInterval(10);

ModelTraining trainer = new SampleClassification()
    // data
	.setFeatures(inputFeatures)
	.setLabels(dataset.labels())
	.setTrainingSamples(nodes.range(0, 0.6))
	.setValidationSamples(nodes.range(0.6, 0.8))
	// learning strategy
	.setOptimizer(new Adam(0.01))
	.setEpochs(3000)
	.setPatience(100)
	.setLoss(new CategoricalCrossEntropy())
	.setValidationLoss(loggingLoss);

Model model = modelBuilder.getModel()
	.init(new XavierNormal())
	.train(trainer);</code></pre>


	<details><summary>About the example training strategy.</summary>

	<p>Of data needed for training, the graph adjacency matrix and node features are already declared as constants by the
	<code class="language-java">FastBuilder</code> constructor, as node classification takes place on the same graph
	with fully known node features. Thus, input features are a column of node identifiers, which the
	<code>classify</code> method uses to gather
	the predictions on respective nodes. Architecture outputs are softmax approximation of the one-hot
	encodings of respective node labels. The simplest way to handle missing labels for test data without modifying
	the example is to leave their one-hot encodings as zeroes only.
	Additionally, this particular training strategy accepts training and validation data slices, where slices are lists
	of integer entries pointing to rows of inputs and outputs - find more later.</p>


	<p>The example further selects
	<code class="language-java">Adam</code> optimization with learning rate <i>0.01</i>, and training
	over many epochs with early stopping. A verbose
	loss prints every 10 epochs the progress of cross entropy and accuracy on validation data, where the
	first of these two is used for the early stopping criterion.
	To run a full training process, pass a strategy to a model.
	In a cold start scenario, apply a parameter initializer first before training is conducted.
	A warm start that resumes training from some previously trained outcomes would skip this step.
	Selecting an initializer is not part of the learning strategy
	to signify its model-dependent nature; dense layers should maintain the expected
	input variances in the output before the first epoch, and therefore the initializer depends
	on the type of activation functions.
	</p>
	</details>
	
	<p>Trained models and their generating builders can be saved and loaded. The next snippet demonstrates
	how raw predictions can be made too. During this process,
	some matrix manipulation operations are employed to obtain transparent access to parts of input and output data
	of the dataset without copying memory.
	</p>

	<pre><code class="language-java">modelBuilder.save(Paths.get("gcn_cora.jgnn")); // needs a Path as an input
Model loadedModel = ModelBuilder.load(Paths.get("gcn_cora.jgnn")).getModel();

Matrix output = loadedModel
	.predict(Tensor.fromRange(0, nodes.size()).asColumn())
	.get(0) // get our one output from the list of outputs
	.cast(Matrix.class); // functional cast
double acc = 0;
for(Long node : nodes.range(0.8, 1)) {
	Matrix nodeLabels = dataset.labels().accessRow(node).asRow();
	Tensor nodeOutput = output.accessRow(node).asRow();
	acc += nodeOutput.argmax()==nodeLabels.argmax()?1:0;
}
System.out.println("Acc\t "+acc/nodes.range(0.8, 1).size());</code></pre>
	
    </section>
		

	<section id="gnn-builders">
	<h1>3. GNN Builders</h1>
	<p>We already touched on the subject of model builders in the previous section,
	where one of them was used to create a model. There exist different kinds of
	builders that offer different conveniences. First is a base class for parsing simple
	Neuralang expressions. Second is a fast builder that contains GNN declation boilerplate.
	Last is a full parser for Neuralang that maintains model definitions in a string
	or file while interacting with Java code. Hwe three corresponding builder classes are covered.
	We also summarize debugging mechanisms for
	checking the integrity of constructed models, visualize their data flow, and monitor specific
	data at runtime.</p>
	
	<section id="modelbuilder">
	<h3 id="modelbuilder">3.1. ModelBuilder</h3>
	<p>This is the base model builder class; it offers a wide breadth of functionalities that other builders extend. Models
	take tensors as input and outputs. Tensors will be covered later; for now, it suffices to think of them as
	numerical vectors, which are sometimes endowed with matrix dimensions. The models themselves are built from Java classes
	that indicate sub-operations. However, this can be too verbose many lines of code are needed to declare even simple expressions,
	making models cumbersome to read and maintain - hence the need for
	builders that construct the models from concise symbolic expressions.</p>
	<p>To create a model with the <code class="language-java">ModelBuilder</code> class,
	instantiating the builder, use a method chain to declare an input variable
	with the <code class="language-java">.var(String)</code> method, parse an expression with the 
	<code class="language-java">.operation(String)</code> method, and finally declare which symbol holds
	outputs with the <code class="language-java">.out(String)</code> method.
	The first and last of these methods can be called multiple times
	to declare several inputs and outputs. Inputs need to be only one symbol, but a whole expression
	for evaluation can be declared in outputs. Obtain the created model's instance with the <code class="language-java">.getModel()</code> method.
	</p>

	<p>
	After defining models, use them to make predictions like below.
	The prediction method takes as input one or more comma-separated tensors that match the model's
	inputs (in the same order) and computes a list of output tensors. If inputs are dynamically created,
	an overloaded version of the same method supports passing an array list of input tensors instead.
	</p>
	
	<pre><code class="language-java">ModelBuilder modelBuilder = new ModelBuilder()
	.var("x")
	.operation("y = log(2*x+1)")
	.out("y");
Model model = modelBuilder.getModel();
System.out.println(model.predict(Tensor.fromDouble(2)));
</code></pre>

	<details><summary>Equivalent implementation without the builder.</summary>
		<p>Under the hood, JGNN models are collections of <code class="language-java">NNOperation</code> instances, each representing a numerical computation with
		specified inputs and outputs that are subclasses of
		JGNN's base <code class="language-java">Tensor</code> type. Computations and the state of operations are thread-safe, so that the same model
		can run and be trained in multiple threads simultaneously. This guidebook does not list operation subclasses, as they are rarely used directly and can be found
		in the Javadoc under the modules
		<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/inputs/package-summary.html" target="_blank">nn.inputs</a>,
		<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/activations/package-summary.html" target="_blank">nn.activations</a>,
		and
		<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/nn/pooling/package-summary.html" target="_blank">nn.pooling</a>.
		Create models in pure Java like the example below, which does not have any trainable parameters.
		</p>

		<pre><code class="language-java">Variable x = new Variable();
Constant c1 = new Constant(Tensor.fromDouble(1)); // holds the constant "1"
Constant c2 = new Constant(Tensor.fromDouble(2)); // holds the constant "2"
NNOperation mult = new Multiply()
	.addInput(x)
	.addInput(c2);
NNOperation add = new Add()
	.addInput(mult)
	.addInput(c1);
NNOperation y = new Log()
	.addInput(add);
Model model = new Model()
	.addInput(x)
	.addOutput(y);
System.out.println(model.predict(Tensor.fromDouble(2))); // one-element input
		</code></pre>
		</details>

	<details><summary>Differences between expression parsing and Neuralang.</summary>
	<p>
	The <code>operation</code> method parses string expressions that are typically structured
	as assignments to symbols; the right-hand side of assignments accepts several operators and functions that 
	are listed in the next table. Models allow multiple operations too, which are parsed through either multiple 
	method calls or by being separated with a semicolon <code>;</code> within larger string expressions.
	</p>
	<p>

	All methods need to use previously declared symbols. For example, parsing <code class="language-java">.out("symbol")</code>
	throws an exception if no operation previously assigned to the symbol or declared it as an input. As a safety
	mechanism against consequenses of the parsing engine that conflict with typical programming logic
	<b>symbols cannot be overwritten or set to updated values outside of Neuralang functions</b>.
	</p>
	<p>
	The base model builder
	class does support a roundabout declaration of Neuralang functions with expressions like the following snippet taken from the Quickstart:
	<code  class="language-java">.function("gcnlayer", "(A,h){return A@(h@matrix(?, hidden, reg))+vector(?);}")</code>.
	The first method argument is the declared function symbol's name, and the second should necessarily have the arguments enclosed in
	a parenthesis and the function's body enclosed in brackets. Learn more about Neuralang functions in the language's description
	in the namesake section.
	</p>
	</details>

	<details><summary>Table of parsable expressions.</summary>

	<p>
	Here is a table of available operations that you can use in expressions. Standard
	priority rules for priority and parentheses apply.
	Prefer using configuration hyperparameters
	to set matrix and vector creation, as these transfer their names to respective dimensions for error
	checking - more on this later.
	</p>

    <table class="table" border=" 1">
            <tr>
                <th>Symbol</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><code>x = expr</code></td>
                <td>Operator</td>
                <td>Assign to variable <code>x</code> the outcome of executing expression <code>expr</code>. This expression does not evaluate to anything.</td>
            </tr>
            <tr>
                <td><code>x + y</code></td>
                <td>Operator</td>
                <td>Element-by-element addition.</td>
            </tr>
            <tr>
                <td><code>x * y</code></td>
                <td>Operator</td>
                <td>Element-by-element multiplication.</td>
            </tr>
            <tr>
                <td><code>x - y</code></td>
                <td>Operator</td>
                <td>Element-by-element subtraction.</td>
            </tr>
            <tr>
                <td><code>x @ y</code></td>
                <td>Operator</td>
                <td>Matrix multiplication.</td>
            </tr>
            <tr>
                <td><code>x | y</code></td>
                <td>Operator</td>
                <td>Row-wise concatenation of <code>x</code> and <code>y</code>.</td>
            </tr>
            <tr>
                <td><code>x [y]</code></td>
                <td>Operator</td>
                <td>Gathers the rows of <code>x</code> with indexes <code>y</code>. Indexes are still tensors, whose elements are cast to integers during this operation.</td>
            </tr>
            <tr>
                <td><code>transpose(A)</code></td>
                <td>Function</td>
                <td>Transposes matrix <code>A</code>.</td>
            </tr>
            <tr>
                <td><code>log(x)</code></td>
                <td>Function</td>
                <td>Applies a logarithm on each element of tensor <code>x</code>.</td>
            </tr>
            <tr>
                <td><code>exp(x)</code></td>
                <td>Function</td>
                <td>Exponentiates each element of tensor <code>x</code>.</td>
            </tr>
            <tr>
                <td><code>nexp(x)</code></td>
                <td>Function</td>
                <td>Exponentiates each non-zero element of tensor <code>x</code>. Typically used for neighbor attention (see below).</td>
            </tr>
            <tr>
                <td><code>relu(x)</code></td>
                <td>Function</td>
                <td>Apply relu on each tensor element.</td>
            </tr>
            <tr>
                <td><code>tanh(x)</code></td>
                <td>Function</td>
                <td>Apply a tanh activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>sigmoid(x)</code></td>
                <td>Function</td>
                <td>Apply a sigmoid activation on each tensor element.</td>
            </tr>
            <tr>
                <td><code>dropout(x, rate)</code></td>
                <td>Function</td>
                <td>Apply training dropout on tensor <code>x</code> with constant dropout rate hyperparameter <code>rate</code>.</td>
            </tr>
            <tr>
                <td><code>drop(x, rate)</code></td>
                <td>Function</td>
                <td>Shorthand notation <code>dropout</code>.</td>
            </tr>
            <tr>
                <td><code>lrelu(x, slope)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with constant negative slope hyperparameter <code>slope</code>.</td>
            </tr>
            <tr>
                <td><code>prelu(x)</code></td>
                <td>Function</td>
                <td>Leaky relu on tensor <code>x</code> with learnable negative slope.</td>
            </tr>
            <tr>
                <td><code>softmax(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a softmax reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>sum(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a sum reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>mean(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a mean reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>L1(x, dim)</code></td>
                <td>Function</td>
                <td>Apply an L1 normalization on <code>x</code> across dimension <code>dim</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>L2(x, dim)</code></td>
                <td>Function</td>
                <td>Apply an L2 normalization on <code>x</code> across dimension <code>dim</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>max(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a max reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>min(x, dim)</code></td>
                <td>Function</td>
                <td>Apply a min reduction on <code>x</code>, where <code>dim</code> is either <code>dim:'row'</code> (default) or <code>dim:'col'</code>.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions.</td>
            </tr>
            <tr>
                <td><code>matrix(rows, cols, reg)</code></td>
                <td>Function</td>
                <td>Generate a matrix parameter with respective hyperparameter dimensions, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
            <tr>
                <td><code>mat(rows, cols)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>matrix</code>.</td>
            </tr>
            <tr>
                <td><code>mat(rows, cols, reg)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>matrix</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter <code>len</code>.</td>
            </tr>
            <tr>
                <td><code>vector(len, reg)</code></td>
                <td>Function</td>
                <td>Generate a vector with size hyperparameter <code>len</code>, and L2 regularization hyperparameter <code>reg</code>.</td>
            </tr>
            <tr>
                <td><code>vec(len)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>vector</code>.</td>
            </tr>
            <tr>
                <td><code>vec(len, reg)</code></td>
                <td>Function</td>
				<td>Shorthand notation <code>vector</code>.</td>
            </tr>
        </tbody>
    </table></details>

	<p>Model definitions have so far been too simple to be employed in practice;
	we need trainable parameters, which are created inline with the <code>matrix</code>
	and <code>vector</code> Neuralang functions. Do not use equivalent Java code, because
	it is better to keep model definitions simple.
	Additionally, there may be constants and configuration hyperparameters. Of these, constants reflect
	untrainable tensors and set in a builder with <code class="language-java">.const(String, Tensor)</code>.
	Both numbers in the last snippet's symbolic definition are internally parsed into constants.
	</p>
	<p>
	On the other hand, configuration hyperparameters are numerical values used by the parser and
	set for a builder with <code class="language-java">.config(String, double)</code>. Provide another
	configuration's name as the second argument to copy its value.
	On the other hand, hyperparameters can be used as arguments to dimension sizes and regularization.
	Retrieve previously set builder hyperparameters though
	<code class="language-java">.getConfigOrDefault(String, double)</code>, where the second argumement may be ommitted.
	This is mostly useful for bringing into code hyperparameters declared in Neuralang scripts.
	</p>
	</section>

	<section id="fastbuilder">
	<h3 id="fastbuilder">3.2. FastBuilder</h3>
	<p>The <code class="language-java">FastBuilder</code> class
	extends the base model builder with boilerplate methods for setting inputs, outputs, and layers tailored
	to specifif downstream tasks. Prefer this builder to keep trackof the whole model definition in one place within Java code.
	The main difference compared to the base <code class="language-java">ModelBuilder</code>
	is that now we have two constuctor arguments, namely a square matrix <code class="language-java">A</code> that
	is typically a normalization of the (sparse) adjacency matrix, 
	and a feature matrix <code>h0</code>. 
	This builder further supports the notation <code>symbol{l}</code>,
	where the layer counter replaces the symbol part <code>{l}</code> with 0 for the first layer,
	1 for the second, and so on.
	</p>
	<p>
	Prefer the notation <code>h{l}</code> to refer to the node representation
	matrix of the current layer; for the first layer, this is parsed as <code>h0</code>, which is the constant
	set by the constructor. This builder also offers a <code class="language-java">.layer(String)</code>
	method that is a variation of operation parsing where the
	the symbol segment <code>{l+1}</code> is substituted with the next layer's counter,
	the expression is parsed, and the layer counter is incremented by one. Example usage is shown below, where 
	symbolic expressions read similarly to what you would find in a paper.
	</p>

	<p>The base operation of message passing GNNs, which are often used for node classification,
	is to propagate node representations to neighbors via graph edges. Then, neighbors aggregate
	the received representation, where aggregation typically consists of a weighted average per
	the normalized adjacency matrix's edge weights. For symmetric normalization, the
	weighted sum is compatible with spectral graph signal processing. The operation to perform
	one propagation can be written as <code class="language-java">.layer("h{l+1}=A @ h{l}")</code>.
	The propagation's outcome is typically transformed further by passing through a dense
	layer.</p>

	<pre><code class="language-java">FastBuilder modelBuilder = new FastBuilder(adjacency, features)  // sets A, h0
	.layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden, reg))+vector(hidden))")  // h1 = ...
	.layer("h{l+1}=A@(h{l}@matrix(hidden, classes, reg))+vector(classes)"); // h2 = ...
	</code></pre>

	<details><summary>Improving representation diffusion at the cost of resources.</summary>
	<p> Several improvements  of the representation diffusion scheme demonstrated above have been proposed.
	However, these tend to incur marginal accuracy improvements at the cost of a lot of compute and memory.
	</p>
	<p>
	JGNN still supports the improvements listed below, since they could be used when
	running time is not a pressing issue (e.g., for transfer or stream learning that applies updates
	for a few epochs), or to analyse smaller graphs: </p>
	
	<ul>
		<li><b>Edge dropout</b> - Applying dropout on the adjacency
		matrix on each layer with <code class="language-java">.layer("h{l+1}=dropout(A,0.5) @ h{l}")</code>. Usage of this operation seems innocuous,
		but it disables a bunch of caching optimizations that occur under-the-hood.</li>
		<li><b>Heterogeneity</b> - Some rcent approaches explicitly account for high-pass frequency diffusion by accounting for the graph Laplacian too. Insert this into the
		architecture as a normal constant like so: <code class="language-java">.constant("L", adjacency.negative().cast(Matrix.class).setMainDiagonal(1))</code></li>
		<li><b>Edge attention</b> - Performs the dot product of edge nodes to create new edge weights
		per the mathematical formula <code>A.(h<sup>T</sup>h)</code>, where
			<code>A</code> is a sparse adjacency matrix, the dot
			<code>.</code> represents the Hadamard product
			(element-by-element multiplication), and
			<code>h</code> is a dense matrix whose rows hold
			respective node representations. JGNN efficiently implements this operation with
			the Neuralang function <code class="language-java">att(A, h)</code>. For
			example, weighted adjacency matrices for each layer of gated attention networks
			are implemented as <code class="language-java">.operation("A{l} = L1(nexp(att(A, h{l})))")</code>.
		</li>
		<li><b>General message passing</b> - JGNN also supports the the fully generized
	message passing scheme between node neighbors of more complex relational analysis
	<a href="https://arxiv.org/pdf/2202.11097.pdf">[Velickovic, 2022]</a>.
	In this generalization, each edge is responsible for appropriately
	transforming and propagating representations to node neighbors;
	create message matrices whose rows correspond to edges and
	columns to edge features by gathering the features of the edge 
	source and destination nodes. Programmatically,obtain edge source indexes
	<code class="language-java">src=from(A)</code> and destination indexes
	<code class="language-java">dst=to(A)</code>, where <code class="language-java">A</code> is
	the adjacency matrix. Then use the horizontal concatenation operation 
	<code>|</code> to concatenate node features.
	One may also concatenate edge features. Given a constructed message, 
	define any kind of ad-hoc
	mechanism or neural processing of messages with
	traditional matrix operations (take care to define correct matrix sizes 
	for dense transformations, e.g., twice the number of
	columns as <code>h{l}</code> in the previous
	snippet). For any kind of <code class="language-java">LayeredBuilder</code>, 
	don't forget that <code">message{l}</code> within 
	operations is needed to obtain a message from the representations 
	<code>h{l}</code> that is not accidentally shared with future layers.
	Receiver mechanisms need to perform some kind of
	reduction on messages. JGNN implements summation
	reduction, given that this has the same
	theoretical expressive power as maximum-based
	reduction but is easier to backpropagate
	through. Perform this like below. 
	The sum is weighted per the values of
	the adjacency matrix <code>A</code>. Thus,
	perform adjacency matrix normalization only if
	you want such weighting to occur.
	

		<pre><code class="language-java">modelBuilder
	.operation("src=from(A)")
	.operation("dst=to(A)")
	.operation("message{l}=h{l}[src] | h{l}[dst]") // has two times the number of h{l}'s features
	.operation("transformed_message{l}=...") // fill in the transformation
	.operation("received{l}=reduce(transformed_message{l}, A)");</code></pre>
		</li>
	</ul>

	</details>
	
	
	<p>So far, we discussed the propagation mechanisms of
	GNNs, which consider the features of all nodes. However,
	in node classification settings, training data labels
	are typically available only for certain nodes, despite 
	all node features being required to make any prediction. 
	We thus need a mechanism to retrieve the predictions of the top
	layer for those nodes, for example before applying a softmax.
	This is achieved in the snippet below, which uses the gather
	operations through brackets. Alternatively, chain the
	<code class="language-java">FastBuilder.classify()</code> 
	method, which injects this exact code.

	<pre><code class="language-java">modelBuilder
	.var("nodes")
	.layer("h{l} = softmax(h{l})")
	.operation("output = h{l}[nodes]")
	.out("output");</code></pre>

	<p>
	It is often the case that GNNs need to make predictions for the
	whole graph - a task that falls under the more general category of learning invariant GNNs.
	In the simplest case, node representations would be
	reduced for each input graph to obtain one representation. However,
	this is a symmetric operation and therefore fails to distinguish between
	the structural positioning of nodes to be pooled, which is often important.
	One computationally light alternative,
	which you can use in your models,
	is sorting nodes based on learned features
	before concatenating their features into one vector for each graph.
	</p>
	<p>
	This process is further simplified by
	keeping the top <em>reduced</em> number of nodes to
	concatenate their features, where the order is
	determined by an arbitrarily selected feature (in our
	implementation: the sorting is made on the last feature, with the previous feature
	being used to break ties, and so on.
	The idea is that the selected feature determines
	<em>important</em> nodes whose information can be
	adopted by others. To implement this scheme, JGNN
	provides independent operations to sort nodes, gather
	node latent representations, and reshape matrices into
	row or column tensors with learnable transformations to
	class outputs. These components are demonstrated in the
	following code snippet:
	</p>

		<pre><code class="language-java">long reduced = 5;  // input graphs need to have at least that many nodes
long hidden = 8;  // many latent dims reduce speed without GPU parallelization

ModelBuilder builder = new LayeredBuilder()        
.var("A")  
.config("features", 1)
.config("classes", 2)
.config("reduced", reduced)
.config("hidden", hidden)
.layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden))+vector(hidden))")
.layer("h{l+1}=relu(A@(h{l}@matrix(hidden, hidden))+vector(hidden))")
.concat(2) // concatenates the outputs of the last 2 layers
.config("hiddenReduced", hidden*2*reduced)  // 2* due to concatenation
.operation("z{l}=sort(h{l}, reduced)")  // z{l} are node indexes
.layer("h{l+1}=reshape(h{l}[z{l}], 1, hiddenReduced)")
.layer("h{l+1}=h{l}@matrix(hiddenReduced, classes)")
.layer("h{l+1}=softmax(h{l}, dim: 'row')")
.out("h{l}");</code></pre>

	<details><summary>Invariant vs equivariant tasks.</summary>
	<p>Tasks like node classification consider
	<b>equivariant</b> GNNs, whose outputs
	follow any node permutations applied on their inputs.
	In simple terms, if the order of node idenfitifiers is modified
	(both in graph adjacency and in node feature matrices),
	the order of rows will be similarly modified for outputs.
	Most JGNN operations are equivariant, and therefore their
	synthesis is also equivariant.
	</p>
	<p>
	However, there
	are cases where created GNNs should be <b>invariant</b>,
	which means that they should create predictions that remain
	the same despite any input permutations. Invariance is
	the property to impose when classifying graphs, where
	one prediction should be made for the whole graph.</p>

	<p>Imposing invariance is simple enough; take an equivariant
	architecture and then apply an invariant operation on top.
	You may want to perform further transformations (e.g., some
	dense layers) afterwards, but the general idea remains
	the same. JGNN offers two types of invariant operations:
	reductions and sort-based pooling covered above. Reductions
	are straightforward to implement
	by taking a dimensionality reduction mechanism (<code>min</code>,
	<code>max</code>, <code>sum</code>, <code>mean</code>) and
	applying it <i>column-wise</i> on the output feature matrix.
	Recall that each row has the features of a different node,
	so reduction yields an one-dimensional vector that,
	for each feature dimension, aggregates feature values across all nodes.
	</p>
	</details>

	<details><summaty>Invariance based on mean pooling.</summary>
		<p>One easy way to create invariant architectures is by averaging
		representations across all nodes. This is inherently less powerful than
		sorting, unless nodes are enriched with positional features.
		An example architecture following
		this principle follows</p>

                <pre><code class="language-java">ModelBuilder builder = new LayeredBuilder()
    .var("A")
    .config("features", nodeLabelIds.size())
    .config("classes", graphLabelIds.size())
    .config("hidden", 16)
    .layer("h{l+1}=relu(A@(h{l}@matrix(features, hidden)))")
    .layer("h{l+1}=relu(A@(h{l}@matrix(hidden, classes)))")
    .layer("h{l+1}=softmax(mean(h{l}, dim: 'row'))")
    .out("h{l}");</code></pre>
    </details>

	</section>
	
	<section id="neuralang">
	<h3 id="neuralang">3.3. Neuralang</h3>
		
	<p>Neuralang scripts consist of functions that declare machine learning
	components. Use a Rust highlighter to cover all keywords.
	Functions correspond to machine learning modules and call each other.
	At their end lies a <code>return</code> statement, which expresses their
	outcome. All arguments are passed by value, i.e., any assignments are
	performed on fresh variable instances.
	</p>
	<p>
	Before explaining how to use the <code class="language-java">Neuralang</code> model builder,
	we present and analyse code that supports a fully functional architecture.
	First, look at the <code>classify</code> 
	function, which for completeness is presented below.
	This takes two tensor inputs: <code>nodes</code> that correspond to identifiers 
	insicating which nodes should be classified (the output has a number of rows equal to the 
	number of identifiers), and a node feature matrix <code>h</code>. 
	It then computes and returns a softmax for the features of the specified nodes. 
	</p>
	<p>
	Aside from main inputs, the function's
	signature also has several configuration values, whose defaults 
	are indicated by a colon <code>:</code> (only configurations have defaults and conversely).
	The same notation is used to 
	set/overwrite configurations when calling functions, as we do for softmax 
	to apply it row-wise. Think of configurations as keyword 
	arguments of typical programming languages, with the difference that
	they control hyperparameters, like dimension sizes or regularization.
	</p>

	<p>Exclamation marks <code>!</code> before numbers broadcast values
	to all subsequent function calls that have configurations with the same
	name. The broadcasted defaults overwrite already existing defaults of configurations with the same
	name anywhere in the code. All defaults are replaced by values explicitly set when calling functions.
	For example, take advantage of this prioritization to force output layer dimensions match your data. Importantly,
	broadcasted values are stored within JGNN's <code class="language-java">Neuralang</code> model
	builder too; this is useful for Java integration, for example to retrieve learning training hyperparameters
	from the model.</p>

		
	<pre><code class="language-rust">fn classify(nodes, h, epochs: !3000, patience: !100, lr: !0.01) {
	return softmax(h[nodes], dim: "row");
}</code></pre>

	<p class="alert alert-warning d-flex align-items-center" role="alert">
		<i class="bi bi-exclamation-triangle-fill me-2 fs-3"></i>
		Write exact values for configurations, as for now no
		arithmetics can be used to compute them. For example,
		setting
		patience:2*50 creates an error.</p>
	</p>

	<p class="alert alert-info d-flex align-items-center" role="alert">
		<i class="bi bi-info-circle-fill me-2 fs-3"></i>
		Conflicting values have the following priority, from strongest to weakest:<br>
		1. Arguments set during the function's call.<br>
		2. The last broadcasted value (include configurations set in Java).<br>
		3. Function signature defaults.
	</p>


	<p>
	Next, let us look at some functions creating the main body of an architecture.
	In that, the questionmark <code>?</code>
	lets the autosize feature of JGNN determine dimension sizes based on a test run.
	The number of classes is unknown as of writting the model, and thus is externally declared
	with the <code>extern</code> keyword to signify that this value should always be provided
	in Java.
	</p>

	<pre><code class="language-rust">fn gcnlayer(A, h, hidden: 64, reg: 0.005) {
	return A@h@matrix(?, hidden, reg) + vector(hidden);
}
fn gcn(A, h, classes: extern) {
	h = gcnlayer(A, h);
	h = dropout(relu(h), 0.5);
	return gcnlayer(A, h, hidden: classes);
}
</code></pre>

	<details><summary>About the architecture.</summary>


	<p>First, <code>gcnlayer</code> accepts
	two parameters: an adjacency matrix <code>A</code> and input feature matrix <code>h</code>.
	The configuration <code>hidden: 64</code> in the functions's signature
	specifies the deafult number of hidden units,
	whereas <code>reg: 0.005</code> is the L2 regularization applied
	during machine learning.
	Finally, the function returns the activated output of a
	GCN layer.
	</p>
	<p>
	Similarly, look at the <code>gcn</code> function. This declares
	the GCN architecture and has as configuration the number of output classes.
	The function basically consists of two <code>gcnlayer</code> layers,
	where the second's hidden units are set to the value of output classes.</p>

	</details>


	<p>We now move to parsing our declarations with the <code class="language-java">Neuralang</code>
	model builder and using them to create an architecture. To this end, save your code
	to a file and get it as a path <code class="language-java">Path architecture = Paths.get("filename.nn");</code>.
	Avoid external files by inlining the definition within Java code through
	a multiline string per <code class="language-java">String architecture = """ ... """;</code>.
	Below, this string is parsed within a chain, where
	each method call returns the model builder instance to continue calling more methods.
	</p>

	<pre><code class="language-java">long numSamples = dataset.samples().getSlice().size();
long numClasses = dataset.labels().getCols();
ModelBuilder modelBuilder = new Neuralang()
	.parse(architecture)
	.constant("A", dataset.graph())
	.constant("h", dataset.features())
	.var("nodes")
	.config("classes", numClasses)
	.config("hidden", numClasses+2)  // custom number of hidden dimensions
	.out("classify(nodes, gcn(A,h))")  // expression to parse into a value
	.autosize(new EmptyTensor(numSamples));

System.out.println("Preferred learning rate: "+modelBuilder.getConfig("lr"));</code></pre>

	<details><summary>About the java side.</summary>
	<p>The above snippet sets remaining hyperparameters
	and overwrites the default value
	for <code class="language-java">"hidden"</code>. It also specifies
	that certain variables are constants, namely the adjacency matrix <code>A</code> and node
	representation <code>h</code>, as well as that node identifiers is a variable that serves
	as the architecture's inpu.
	</p>
	<p>
	There could be multiple inputs, so the distinction of what
	is a constant and what is a variable depends mostly on which quantities change
	during training and is managed by only the Java side of the code.
	In the case of node classification, both the adjacency matrix and
	node features remain constant, as we work in one graph, but this is not
	the same for graph classification where many graphs are encountered.
	</p>
	<p>
	Finally, the definition
	sets an Neuralang expression as the architecture's output
	by calling the <code class="language-java">.out(String)</code> method,
	and applies the <code class="language-java">.autosize(Tensor...)</code> method to infer hyperparameter
	values denoted with <code class="language-java">?</code> from an example input.
	For faster completion of the model, we provide a dataless list of node identifiers as input.</p>
	</details>
	</section>
	
	<section id="debugging">
	<h3 id="debugging">3.4. Debugging</h3>
	<p>JGNN offers high-level tools for debugging
		architectures. Here we cover what diagnostics to run, and how to make
		sense of error messages to fix erroneous
		architectures.
	</p>
	<p>
		We already mentioned that model builder
		symbols should be assigned to before
		subsequent use. For example, consider a <code class="language-java">FastBuilder</code> that
		tries to parse the expression <code class="language-java">.layer("h{l+1}=relu(hl@matrix(features, 32, reg)+vector(32))")</code>, 
		where <code>hl</code> is a typographical error of
		<code>h{l}</code>. In this case, an exception is thrown:
		<code>Exception in thread "main" java.lang.RuntimeException: Symbol hl not defined.</code>
	</p>
	<p>
		Internally, models are effectively directed acyclic graphs (DAGs)
		that model builders create. DAGs should not be confused with the graphs
		that GNNs architectures analyse; they are just an organization of data flow
		between <code>NNComponent</code>s. During parsing, builders
		may create temporary variables, which start with
		the <code>_tmp</code> prefix and are followed by
		a number. Those variables often link
		components to others that use them.
	</p>
	<p>
		The easiest way to understand execution DAGs is
		to look at them textually or visually: a <code class="language-java">.print()</code> method
		that prints built functional flows in the system
		console, and a <code class="language-java">.getExecutionGraphDot()</code> 
		method that returns a string holding the execution graph in
		<em>.dot</em> format for visualization with
		tools like <a href="https://dreampuf.github.io/GraphvizOnline" target="_blank">GraphViz</a>.
		An example is shown below.
	</p>

	<img src="graphviz.png" alt="Graphviz Image" width="100%"/>
	<br>
	<br>

	<p>Another error-checking procedure consists of
		an assertion that all model operations eventually affect
		at least one output. Computational branches that lead nowhere mess up the
		DAG traversal during backpropagation and should be checked with the
		method <code class="language-java">.assertBackwardValidity()</code>.
		The latter throws an exception if an invalid model is found.
		Backward validity errors
		look like this the following example. This 
		indicates that the component
		<code class="language-java">_tmp102</code> does not lead to an output,
		and we should look at the execution tree to
		understand its role.</p>

	<pre><code>Exception in thread "main" java.lang.RuntimeException: The component class mklab.JGNN.nn.operations.Multiply: _tmp102 = null does not lead to an output
at mklab.JGNN.nn.ModelBuilder.assertBackwardValidity(ModelBuilder.java:504)
at nodeClassification.APPNP.main(APPNP.java:45)</code></pre>


	<p class="alert alert-warning d-flex align-items-center" role="alert">
		<i class="bi bi-exclamation-triangle-fill me-2 fs-3"></i>
		Asserting backwards validity before fully declaring a model
		will likely throw exceptions, as independend
		outputs may be combined later.
	</p>


	<p>Some tensor or matrix methods do not
	correspond to numerical operations but
	are only responsible for naming dimensions.
	</section>
	Functionally, such methods are largely decorative,
	but they improve debugging by throwing errors for
	incompatible non-null names. For example,
	adding two matrices with different dimension
	names throws an exception. Likewise, the
	inner dimension names during matrix
	multiplication should agree.
	Arithmetic operations, such as
	matrix multiplication and copying,
	automatically infer dimension names in the
	result to ensure that only compatible data types
	are compared. Dimension name changes
	do <em>not</em>
	backtrack the changes, even for see-through
	data types, such as the outcome of
	<code class="language-java">asTransposed()</code>.
	Matrices effectively have three
	dimension names: for their rows, columns,
	and inner data as long as they are treated
	as tensors.</p>

	<details><summary>How to manually set dimension names.</summary>

	<table class="table table-bordered">
		<thead>
			<tr>
				<th>Operation</th>
				<th>Comments</th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td><code class="language-java">Tensor setDimensionName(String name)</code></td>
				<td>For naming tensor dimensions (of
					the 1D space tensors lie
					in).</td>
			</tr>
			<tr>
				<td><code class="language-java">Tensor setRowName(String rowName)</code></td>
				<td>For naming what kind of
					information matrix rows hold
					(e.g., "samples"). Defined only to matrices.</td>
			</tr>
			<tr>
				<td><code class="language-java">Tensor setColName(String colName)</code></td>
				<td>For naming what kind of
					information matrix columns hold
					(e.g., "features"). Defined only for matrices.</td>
			</tr>
			<tr>
				<td><code class="language-java">Tensor setDimensionName(String rowName, String colName)</code></td>
				<td>A shorthand of calling
					<code class="language-java">setRowName(rowName).setColName(colName)</code>. Defined only for matrices.
				</td>
			</tr>
		</tbody>
	</table>
	</details>

	<p>There are two main mechanisms for identifying
	logical errors within architectures: a) mismatched
	dimension size, and b) mismatched dimension
	names. Of the two, dimension sizes are easier to
	comprehend since they just mean that operations
	are mathematically invalid.
	On the other hand, dimension names need to be
	determined for starting data, such as model
	inputs and parameters, and are automatically
	inferred from operations on such primitives. For
	in-line declaration of parameters in operations
	or layers, dimension names are copied from any
	hyperparameters. Therefore, for easier
	debugging, prefer using functional expressions
	that declare hyperparameters, similarly to the example.</p>

	<pre><code class="language-java">new ModelBuilder()
	.config("features", 7)
	.config("hidden", 64)
	.var("x")
	.operation("h = x@matrix(features, hidden)");</code></pre>

	<p>Both mismatched dimensions and mismatched
		dimension names throw runtime exceptions. The
		beginning of their error console traces should
		start with something like this:</p>

	<pre><code>java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null with the following inputs:
class mklab.JGNN.nn.activations.Relu: h1 = SparseMatrix (3327,32) 52523/106464 entries
class mklab.JGNN.nn.inputs.Parameter: _tmp5 = DenseMatrix (64, classes 6)
java.lang.IllegalArgumentException: Mismatched matrix sizes between SparseMatrix (3327,32) 52523/106464 entries and DenseMatrix (64, classes 6)
at mklab.JGNN.core.Matrix.matmul(Matrix.java:258)
at mklab.JGNN.nn.operations.MatMul.forward(MatMul.java:21)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:180)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
at mklab.JGNN.nn.NNOperation.runPrediction(NNOperation.java:170)
	...</code></pre>

	<details><summary>How to read the error message.</summary>
	<p>This particular stack trace tells us that the architecture
	encounters mismatched matrix sizes when trying
	to multiply a 3327x32 SparseMatrix with a 64x6
	dense matrix. Understanding the exact error is
	easy—the inner dimensions of matrix
	multiplication do not agree. However, we need to
	find the error within our architecture to fix
	it. To do this, the error message message states 
	that the error occures.
	<code>During the forward pass of class mklab.JGNN.nn.operations.MatMul: _tmp4 = null</code>.
	</p>
	<p>
	This tells us that the problem
	occurs when trying to calculate
	<code>_tmp4</code>, which is currently assigned
	a <code class="language-java">null</code> tensor as value. Some more
	information is available to see what the
	operation's inputs are like—in this case, they
	coincide with the multiplication's inputs, but
	this will not always be the case.
	The important point is to go back to the
	execution tree and see during which exact
	operation this variable is defined. There, we
	will undoubtedly find that some dimension had 64
	instead of 32 elements or conversely, and we
	can fix the parsed expression responsible.</p>
	</details>

	<p>In addition to all other debugging mechanisms,
	JGNN presents a way to show when forward and
	backward operations of specific code components
	are executed and with what kinds of arguments.
	This can be particularly useful when testing new
	components in real (complex) architectures.
	The practice consists of calling a
	<code class="language-java">monitor(...)</code> function within
	operations. This does not affect what
	expressions do and only enables printing
	execution tree operations on operation
	components. For example, the next snippet
	monitors the outcome of matrix multiplication:
	</p>
	<pre><code class="language-java">builder.operation("h = relu(monitor(x@matrix(features, 64)) + vector(64))")</code></pre>
	</section>
	</section>

	<section id="training">
		<h1>4. Training</h1>
		<p>Here we describe how to train JGNN models created
		with the previous section's builders.
		Broadly, we need to load some reference data and employ an optimization scheme
		to adjust trainable parameter values based on the differences between desired
		and current outputs. We
		desribe generic patterns before moving to specific tasks. For the latter,
		helper classes that implement common training schemas.</p>


		<p class="alert alert-info d-flex align-items-center" role="alert">
			<i class="bi bi-info-circle-fill me-2 fs-3"></i>
		Request more or improved helper classes in the project's GitHub issues.</p>


		<section id="create-data">
		<h3 id="create-data">4.1. Create data</h3>
		<p>JGNN contains dataset classes that automatically download and load 
		datasets for out-of-the-box experimentation. These datasets can be found
		in the <a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/adhoc/datasets/package-summary.html" target="_blank">
		adhoc.datasets</a> Javadoc, and we already covered their usage patterns. 
		</p>
		<p>
		In practice, though, you will want to 
		use your own data. In the simplest case, both the number of nodes or data samples, and
		the number of feature dimensions are known beforehand. If so, create
		dense feature matrices with the following code. The example below uses the
		minimum memory necessary to construct the feature matrix. If
		features are dense (do not have many zeros),
		consider using the <code class="language-java">DenseMatrix</code> class
		instead of initializing a sparse matrix. Creating adjacency matrices is
		similar to creating preallocated feature matrices - just make sure that they are square,
		that is, have the same number of rows and columns. </p>

			<pre><code class="language-java">Matrix features = new SparseMatrix(numNodes, numFeatures);
for(long nodeId=0; nodeId&lt;numNodes; nodeId++)
	for(long featureId=0; featureId&lt;numFeatures; featureId++)
		features.put(nodeId, featureId, 1);</code></pre>
		</section>

		<p>All tensor operations can be viewed in the
		<a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/core/Tensor.html" target="_blank">core.tensor</a>
		and <a href="https://mklab-iti.github.io/JGNN/javadoc/mklab/JGNN/core/Matrix.html" target="_blank">core.matrix</a>
		Javadoc. The <code class="language-java">Matrix</code> class extends the concept
		of tensors with additional operations, like transposition, matrix multiplication,
		and row and column access. Under the
		hood, matrices linearly store elements and use
		computations to transform the (row, col)
		position of their elements to respective
		positions. The outcome of some methods inherited
		from tensors may need to be typecast back into a
		matrix (e.g., for all in-place operations).
		</p>

		<details><summary>Constructing data tables line-by-line.</summary>
		<p>Sometimes, it is easier to read node or sample features
		line-by-line, for instance, when reading a <em>.csv</em>
		file. In this case, store each line as a separate tensor.
		Convert a list of tensors representing row vectors into a
		feature matrix like in the example below.</p>

			<pre><code class="language-java">ArrayList<Tensor> rows = new ArrayList<Tensor>();
try(BufferedReader reader = new BufferedReader(new FileReader(file))){
	String line = reader.readLine();
	while (line != null) {
		String[] cols = line.split(",");
		Tensor features = new SparseTensor(cols.length);
		for(int col=0;col&lt;cols.length;col++)
			features.put(col, Double.parseDouble(cols[col]));
		rows.add(features);
		line = reader.readLine();
	}
}
Matrix features = new WrapRows(rows).toSparse();</code></pre>
		</details>

		<details><summary>Which matrix type is appropriate for representing graphs?</summary>
		<p>When in doubt, use the <b>sparse</b>
		format for adjacency matrices, as the allocated memory of dense
		counterparts scales qudratically to the number of nodes. Note that many GNNs
		consider bidirectional (i.e., non-directed) edges, in which case
		both directions should be added to the adjacency. Use the following snippet as a
		template. Recall that JGNN follows a function chain notation, so each modification
		returns the <code class="language-java">matrix</code> instance.
		Do not forget to normalize or apply the renormalization trick (self-edges) on matrices if these
		are needed by your architecture, for instance by calling
		<code class="language-java">adjacency.setMainDiagonal(1).setToSymmetricNormalization();</code>
		after matrix creation.</p>

		<pre><code class="language-java">Matrix adjacency = new SparseMatrix(numNodes, numNodes);
for(Entry&lt;Long, Long&gt; edge : edges)
	matrix
		.put(edge.getKey(), edge.getValue(), 1)
		.put(edge.getValue(), edge.getKey(), 1);</code></pre>
		</details>

		<details><summary>About Java-side tensor and matrix operations.</summary>
		<o>
		Operations can be split into arithmetics that combine the values
		of two tensors to create a new one (e.g., <code class="language-java">Tensor add(Tensor)</code>),
		in-place arithmetics that alter a tensor without creating
		a new one (e.g., <code class="language-java">Tensor selfAdd(Tensor)</code>),
		summary statistics that output simple numeric values (e.g., <code class="language-java">double Tensor.sum()</code>),
		and element getters and setters.
		</p>
		<p>
		In-place arithmetics follow the same naming
		conventions of base arithmetics but their method names begin with a "self"
		prefix for pairwise operations and a "setTo" prefix
		for unary operations. Since they do not allocate new memory,
		prefer them for intermediate calculation steps.
		For example, the following code can be
		used for creating and normalizing a tensor of
		ones without using any additional memory.</p>

		<pre><code class="language-java">Tensor normalized = new DenseTensor(10)
	.setToOnes()
	.setToNormalized();</code></pre>

		<p>Initialize a dense or sparse tensor -both of which represent one-dimensional vectors- with its number
		of elements. If there are many zeros expected,
		prefer using a sparse tensor. For example, one-hot encodings for classification
			problems can be generated with the following
			code. This creates a dense tensor with
			<code class="language-java">numClasses</code> elements and puts at
			element <code class="language-java">classId</code> the value 1:
		</p>

		<pre><code class="language-java">int classId = 1;
int numClasses = 5;
Tensor oneHotEncoding = new mklab.JGNN.tensor.DenseTensor(numClasses).set(classId, 1); // creates the tensor [0,1,0,0,0]</code></pre>
		</details>

		<p>The above snippets all make use of numerical node identifiers. To
		manage these, JGNN provides an <code class="language-java">IdConverter</code> class; 
		convert hashable objects (typically strings) to identifiers by calling
		<code  class="language-java">IdConverter.getOrCreateId(object)</code>. Also use
		converters to one-hot encode class labels. To search only for previously
		registered identifiers, use <code class="language-java">IdConverter.get(object)</code>.
		For example, construct a label matrix with the following snippet.
		In this, <code class="language-java">nodeLabels</code> is a dictionary
		from node identifiers to node labels that is being converted to a sparse matrix.</p>

		<pre><code class="language-java">IdConverter nodeIds = new IdConverter();
IdConverter classIds = new IdConverter();
for(Entry&lt;String, String&gt; entry : nodeLabels) {
	nodeids.getOrCreateId(entry.getKey());
	classIds.getOrCreateId(entry.getValue());
}
Matrix labels = new SparseMatrix(nodeIds.size(), classIds.size());
for(Entry&lt;String, String&gt; entry : nodeLabels) 
	labels.put(nodeids.get(entry.getKey()), classIds.get(entry.getValue()), 1);</code></pre>

		<p>Reverse-search the converter to obtain the original object
		of predictions per <code class="language-java">IdConverter.get(String)</code>. The following example
		accesses one row of a label matrix, performs and argmax operation to find the position of the 
		maximum element, and reconstruct the label for the corresponding row with reverse-search.
		</p>

		<pre><code class="language-java">long nodeId = nodeIds.get("nodeName");
Tensor prediction = labels.accessRow(nodeId);
long predictedClassId = prediction.argmax();
System.out.println(classIds.get(predictedClassId));</code></pre>
			
		</section>
		<section id="node-classification">
		<h3 id="node-classification">4.2. Node classification</h3>
		<p>
		Node classification models can be backpropagated by considering a list of node indeces and desired
		predictions for those nodes. We first show an automation of the training process that controls
		it in a predictable manner.
		</p>

		<b>This section is under construction.</b>
		
		<pre><code class="language-java">Slice nodes = dataset.samples().getSlice().shuffle(100);  // or nodes = new Slice(0, numNodes).shuffle(100);
Model model = modelBuilder()
	.getModel()
	.init(new XavierNormal())
	.train(trainer,
			nodes.samplesAsFeatures(), 
			dataset.labels(), 
			nodes.range(0, trainSplit), 
			nodes.range(trainSplit, validationSplit));
		</code></pre>

		</section>

		<section id="graph-classification">
		<h3 id="graph-classification">4.3. Graph classification</h3>


		<p>
		We already saw that, for more general tasks attributed graph function (AGF) approximation
		tasks, like graph classification, equivariant architecttures are needed.
		A simple training mechanism that can train any kind of AGF is presented below.
		In this, it is assumed that there are graph adjacency matrices,
		corresponding node feature matrices,
		and resulting ideal architectural output labels (or any other value set as the ground
		truth of the loss function). These should be organized into lists with the same
		number of elements. An example is shown below.
		</p>

		<pre><code class="language-java">
List&lt;Matrix&gt; graphs = ...;
List&lt;Matrix&gt; features = ...;
List&lt;Matrix&gt; labels = ...;

Loss loss = new VerboseLoss(new CategoricalCrossEntropy(), new Accuracy())
		.setPrintOnImprovement(true));
ModelTraining trainer = new AGFTraining()
		.setGraphs(graphs)
		.setNodeFeatures(features)
		.setGraphLabels(labels)
		.setValidationSplit(0.2)
		.setEpochs(300)
		.setOptimizer(new Adam(0.01))
		.setLoss(new CategoricalCrossEntropy())
		.setNumParallellBatches(10)
		.setValidationLoss(loss);

Model model = builder.getModel()
		.init(new XavierNormal())
		.train(trainer);

double acc = 0.0;
for(int graphId=0; graphId&lt; graphs.size(); graphId++) {
	Matrix adjacency = graphs.get(graphId);
	Matrix nodeFeatures = features.get(graphId);
	Tensor graphLabel = labels.get(graphId);
	if(model.predict(Arrays.asList(nodeFeatures, adjacency)).get(0).argmax()==graphLabel.argmax())
		acc += 1;
}
System.out.println("Test accuracy " + acc/dtest.graphs.size());
		</code></pre>


		<details><summary>About parallelization.</summary>
		<p>
		To speed up graph classification, use JGNN's
		parallelization capabilities to calculate gradients
		across multiple threads. Parallelization for node
		classification holds little meaning, as the same
		propagation mechanism needs to run on the same graph without
		reducing the number of computations.
		However, executing gradient computations in parallel yields
		substantial speedup for many other attributed graph function
		approximation problems, such as graph classification.
		</p>
		<p>
		In the above scheme, parallelization is
		included by calling the method
		<code  class="language-java">setNumParallellBatches(number)</code>,
		which creates a number of batches equal to the number
		of threads and then sets gradient calculations to be computed
		in parallel across all batches. The optimizer accumulates those
		gradients in a thread-safe manner and then applies them together
		once all batch gradients are computed.
		</p>
		<p>
		If you do not want any parallelization, this is disabled by
		default, uses a number of threads equal to the number of batches
		(so set one batch to disable it), and is not toggled if you use
		<code  class="language-java">setNumBatches(number)</code>,
		instead.
		</p>
		</details>


		<details><summary>Custom implementation of training with JGNN'sthread pools.</summary>

			<p>Many scenarios are covered by attributed graph function approximation,
			but it may be more convenient to train architectures
			with custom schemes. This requires manually calling the
			backpropagation for each epoch and each graph in the
			training batch. To do this, first retrieve the model and
			initialize its parameters:</p>

			<pre><code class="language-java">Model model = builder.getModel()
	.init(new XavierNormal());</code></pre>

			<p>Next, define a loss function and set up a batch
				optimization strategy. Wrap any base optimizer and
				accumulating parameter updates until
				<code class="language-java">BatchOptimizer.updateAll()</code> is called later
				on. This is done automatically by training schemas if the provided optimizer
				does not already support batching.
			</p>

			<pre><code class="language-java">Loss loss = new CategoricalCrossEntropy();
BatchOptimizer optimizer = new BatchOptimizer(new Adam(0.01));</code></pre>

			<p>Finally, training can be conducted by iterating through
				epochs and training samples and appropriately calling
				the <code class="language-java">Model.train</code> for combinations of node
				features and graph adjacency matrix inputs and graph
				label outputs. At the end of each batch (e.g., each
				epoch), don't forget to call the
				<code class="language-java">optimizer.updateAll()</code> method to apply the
				accumulated gradients. This process can be realized with
				the following code:
			</p>

			<pre><code class="language-java">for(int epoch=0; epoch&lt;300; epoch++) {
	for(int graphId=0; graphId&lt;graphLabels.size(); graphId++) {
		 Matrix adjacency = graphs.get(graphId);
		 Matrix nodeFeatures = features.get(graphId);
		 Tensor graphLabel = graphLabels.get(graphId);
		 model.train(loss, optimizer, 
			  Arrays.asList(nodeFeatures, adjacency),
			  Arrays.asList(graphLabel));
	}
	optimizer.updateAll();
}</code></pre>

		<p>A maual implementation of parallelization can use JGNN's thread pooling to
		perform gradients, wait for the conclusion of submitted
		tasks, and then apply the accumulated gradient updates. 
		This is achieved through a batch optimizer that accumulates
		gradients.</p>

                <pre><code class="language-java">for(int epoch=0; epoch&lt;500; epoch++) {
    // gradient update
    for(int graphId=0; graphId&lt;graphs.size(); graphId++) {
        int graphIdentifier = graphId;
        ThreadPool.getInstance().submit(new Runnable() {
            @Override
            public void run() {
                Matrix adjacency = graphs.get(graphIdentifier);
                Matrix nodeFeatures = features.get(graphIdentifier);
                Tensor graphLabel = labels.get(graphIdentifier).asRow();
                model.train(loss, optimizer, 
		            Arrays.asList(nodeFeatures, adjacency),
		            Arrays.asList(graphLabel));
            }
        });
    }

	// wait for all gradients before parameter updates
    ThreadPool.getInstance().waitForConclusion();
    optimizer.updateAll();

    double acc = 0.0;
    for(int graphId=0; graphId&lt;adjacency.size(); graphId++) {
        Matrix adjacency = graphs.get(graphId);
        Matrix nodeFeatures = features.get(graphId);
        Tensor graphLabel = labels.get(graphId);
        if(model.predict(Arrays.asList(nodeFeatures, adjacency)).get(0).argmax()==graphLabel.argmax())
           acc += 1;
        System.out.println("iter = " + epoch + "  " + acc/graphs.size());
    }
}</code></pre>
	</details>


    </section>



	<section id="losses-and-optimizers">
	<h3 id="losses-and-optimizers">4.4. Losses and optimizers</h3>

	<p>Training strategies use various loss functions to determine what training objectives
	are and optimizers that determine how trainable model parameters should be updated given
	their current state and computed gradients. You can define both choices with custom classes,
	or use existing ones.
	</p>

	<p>
	A pattern that JGNN provides is to wrap some base loss or optimizer
	with an augmentation that improves its core behavior. For example, you can find
	losses that report on progress either regularly or on epochs where their
	value decreases. Note that training and validation losses can be set differently.
	</p>


	<p class="alert alert-info d-flex align-items-center" role="alert">
		<i class="bi bi-info-circle-fill me-2 fs-3"></i>
		The preferred GNN optimizer in the literature is Adam with 0.01 learning rate.
	</p>


    </section>


    </section>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-rust.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-xml.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>
